{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of The Annotated \"Attention is All You Need\".ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YanglanWang/Attention-is-all-you-need/blob/master/Copy_of_The_Annotated_%22Attention_is_All_You_Need%22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wArePITKUgQG",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"aiayn.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSWEk4ttUgQH",
        "colab_type": "text"
      },
      "source": [
        "> When teaching, I emphasize implementation as a way to understand recent developments in ML. This post is an attempt to keep myself honest along this goal. The recent [\"Attention is All You Need\"]\n",
        "(https://arxiv.org/abs/1706.03762) paper from NIPS 2017 has been instantly impactful paper as a new method for machine translation and potentiall NLP generally. The paper is very clearly written, but the conventional wisdom has been that it is quite difficult to implement correctly. \n",
        ">\n",
        "> In this post I follow the paper through from start to finish and try to implement each component in code. \n",
        "(I have done some minor reordering and skipping from the original paper). This document itself is a working notebook, and should be a completely usable and efficient implementation. To follow along you will first need to install [PyTorch](http://pytorch.org/) and [torchtext](https://github.com/pytorch/text). The complete code is available on [github](https://github.com/harvardnlp/annotated-transformer).\n",
        ">- Alexander \"Sasha\" Rush ([@harvardnlp](https://twitter.com/harvardnlp))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaYyfFUqUnGY",
        "colab_type": "code",
        "outputId": "13eddbc7-53a7-47e8-dfa9-b92afd9276d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        }
      },
      "source": [
        "!pip install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl numpy matplotlib spacy torchtext "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch==0.3.0.post4 from http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl\n",
            "\u001b[?25l  Downloading http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl (592.3MB)\n",
            "\u001b[K     |████████████████████████████████| 592.3MB 1.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.16.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (3.0.3)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.1.6)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.6/dist-packages (0.3.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from torch==0.3.0.post4) (3.13)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.1.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.5.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.0.8)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.4)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.9.6)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.0.7)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.21.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext) (4.28.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib) (41.0.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib) (1.12.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.6.16)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
            "\u001b[31mERROR: torchvision 0.3.0 has requirement torch>=1.1.0, but you'll have torch 0.3.0.post4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fastai 1.0.55 has requirement torch>=1.0.0, but you'll have torch 0.3.0.post4 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch\n",
            "  Found existing installation: torch 1.1.0\n",
            "    Uninstalling torch-1.1.0:\n",
            "      Successfully uninstalled torch-1.1.0\n",
            "Successfully installed torch-0.3.0.post4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LTc4HW7UgQI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Standard PyTorch imports\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math, copy\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# For plots\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_R749nLNUgQL",
        "colab_type": "text"
      },
      "source": [
        "* Table of Contents                               \n",
        "{:toc}      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esxhOQubUgQL",
        "colab_type": "text"
      },
      "source": [
        "# Background"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1M-PiEMOUgQM",
        "colab_type": "text"
      },
      "source": [
        "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n",
        "[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\n",
        "block, computing hidden representations in parallel for all input and output positions. In these models,\n",
        "the number of operations required to relate signals from two arbitrary input or output positions grows\n",
        "in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\n",
        "it more difficult to learn dependencies between distant positions [12]. In the Transformer this is\n",
        "reduced to a constant number of operations, albeit at the cost of reduced effective resolution due\n",
        "to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\n",
        "described in section 3.2.\n",
        "\n",
        "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions\n",
        "of a single sequence in order to compute a representation of the sequence. Self-attention has been\n",
        "used successfully in a variety of tasks including reading comprehension, abstractive summarization,\n",
        "textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\n",
        "End-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned\n",
        "recurrence and have been shown to perform well on simple-language question answering and\n",
        "language modeling tasks [34].\n",
        "\n",
        "To the best of our knowledge, however, the Transformer is the first transduction model relying\n",
        "entirely on self-attention to compute representations of its input and output without using sequencealigned\n",
        "RNNs or convolution. In the following sections, we will describe the Transformer, motivate\n",
        "self-attention and discuss its advantages over models such as [17, 18] and [9]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84vTAA5TUgQM",
        "colab_type": "text"
      },
      "source": [
        "# Model Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-f9BuNsUgQN",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBOvyU9BUgQN",
        "colab_type": "text"
      },
      "source": [
        "Most competitive neural sequence transduction models have an encoder-decoder structure [(cite)](cho2014learning,bahdanau2014neural,sutskever14). Here, the encoder maps an input sequence of symbol representations $(x_1, ..., x_n)$ to a sequence of continuous representations $\\mathbf{z} = (z_1, ..., z_n)$. Given $\\mathbf{z}$, the decoder then generates an output sequence $(y_1,...,y_m)$ of symbols one element at a time. At each step the model is auto-regressive [(cite)](graves2013generating), consuming the previously generated symbols as additional input when generating the next. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AC8KeDJUgQO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    A standard Encoder-Decoder architecture. Base model for this and many \n",
        "    other models.\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
        "        super(EncoderDecoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_embed = src_embed\n",
        "        self.tgt_embed = tgt_embed\n",
        "        self.generator = generator\n",
        "        \n",
        "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
        "        \"Take in and process masked src and target sequences.\"\n",
        "        memory = self.encoder(self.src_embed(src), src_mask)\n",
        "        output = self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ip0EXqvEUgQQ",
        "colab_type": "text"
      },
      "source": [
        "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9dznVhQUgQQ",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"ModalNet-21.png\" width=400px>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euyRXbaMUgQR",
        "colab_type": "text"
      },
      "source": [
        "## Encoder and Decoder Stacks   \n",
        "\n",
        "### Encoder: \n",
        "\n",
        "The encoder is composed of a stack of $N=6$ identical layers. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yy7pY85UgQR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clones(module, N):\n",
        "    \"Produce N identical layers.\"\n",
        "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psiq5idJUgQT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"Core encoder is a stack of N layers\"\n",
        "    def __init__(self, layer, N):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "        \n",
        "    def forward(self, x, mask):\n",
        "        \"Pass the input (and mask) through each layer in turn.\"\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mie9sUeSUgQV",
        "colab_type": "text"
      },
      "source": [
        "We employ a residual connection [(cite)](he2016deep) around each of the two sub-layers, followed by layer normalization [(cite)](layernorm2016).  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEz9kLClUgQV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    \"Construct a layernorm module (See citation for details).\"\n",
        "    def __init__(self, features, eps=1e-6):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.a_2 = nn.Parameter(torch.ones(features))\n",
        "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nx4On5PCUgQY",
        "colab_type": "text"
      },
      "source": [
        "That is, the output of each sub-layer is $\\mathrm{LayerNorm}(x + \\mathrm{Sublayer}(x))$, where $\\mathrm{Sublayer}(x)$ is the function implemented by the sub-layer itself.  We apply dropout [(cite)](srivastava2014dropout) to the output of each sub-layer, before it is added to the sub-layer input and normalized.  \n",
        "\n",
        "To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension $d_{\\text{model}}=512$.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zx9JBwAcUgQY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SublayerConnection(nn.Module):\n",
        "    \"\"\"\n",
        "    A residual connection followed by a layer norm.\n",
        "    Note for code simplicity we apply the norm first as opposed to last.\n",
        "    \"\"\"\n",
        "    def __init__(self, size, dropout):\n",
        "        super(SublayerConnection, self).__init__()\n",
        "        self.norm = LayerNorm(size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        \"Apply residual connection to any sublayer function that maintains the same size.\"\n",
        "        return x + self.dropout(sublayer(self.norm(x)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZduB6mIlUgQa",
        "colab_type": "text"
      },
      "source": [
        "Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mEBw9tIUgQb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    \"Encoder is made up of two sublayers, self-attn and feed forward (defined below)\"\n",
        "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = self_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
        "        self.size = size\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        \"Follow Figure 1 (left) for connections.\"\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
        "        return self.sublayer[1](x, self.feed_forward)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHOZnpnBUgQd",
        "colab_type": "text"
      },
      "source": [
        "### Decoder:\n",
        "\n",
        "The decoder is also composed of a stack of $N=6$ identical layers.  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3o_ZB42sUgQd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    \"Generic N layer decoder with masking.\"\n",
        "    def __init__(self, layer, N):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "        \n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, memory, src_mask, tgt_mask)\n",
        "        return self.norm(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOi_W1qaUgQf",
        "colab_type": "text"
      },
      "source": [
        "In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack.  Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMm6xHWVUgQg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    \"Decoder is made up of three sublayers, self-attn, src-attn, and feed forward (defined below)\"\n",
        "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.size = size\n",
        "        self.self_attn = self_attn\n",
        "        self.src_attn = src_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
        " \n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        \"Follow Figure 1 (right) for connections.\"\n",
        "        m = memory\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
        "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
        "        return self.sublayer[2](x, self.feed_forward)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Nen9h7wUgQi",
        "colab_type": "text"
      },
      "source": [
        "We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions.  This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position $i$ can depend only on the known outputs at positions less than $i$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARUk8yZY0mbt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_ll=np.arange(1,401).reshape(1,20,20)\n",
        "print(test_ll)\n",
        "test_a=np.triu(test_ll,1)\n",
        "print(test_a)\n",
        "a=torch.from_numpy(test_a)==0\n",
        "print(a[0])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyQKI9AgUgQj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def subsequent_mask(size):\n",
        "    \"Mask out subsequent positions.\"\n",
        "    attn_shape = (1, size, size)\n",
        "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
        "    return torch.from_numpy(subsequent_mask) == 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgQMtvM-UgQl",
        "colab_type": "code",
        "outputId": "29b4200f-a916-40c2-8b36-abdee2e3337e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "# The attention mask shows the position each tgt word (row) is allowed to look at (column).\n",
        "# Words are blocked for attending to future words during training. \n",
        "plt.figure(figsize=(5,5))\n",
        "plt.imshow(subsequent_mask(20)[0])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f7fcc7bd240>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAT8AAAEyCAYAAACMONd1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fl2E1IaqUgQq",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3twSbimFUgQq",
        "colab_type": "text"
      },
      "source": [
        "### Attention:                                                                                                                                                                                                                                                                               \n",
        "An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors.  The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.                                                                                                                                                                                                                                                                                           \n",
        "\n",
        "We call our particular attention \"Scaled Dot-Product Attention\".   The input consists of queries and keys of dimension $d_k$, and values of dimension $d_v$.  We compute the dot products of the query with all keys, divide each by $\\sqrt{d_k}$, and apply a softmax function to obtain the weights on the values.                                                                                                         \n",
        "<img width=\"220px\" src=\"ModalNet-19.png\">\n",
        "\n",
        "In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix $Q$.   The keys and values are also packed together into matrices $K$ and $V$.  We compute the matrix of outputs as:                      \n",
        "                                                                 \n",
        "$$                                                                         \n",
        "   \\mathrm{Attention}(Q, K, V) = \\mathrm{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V               \n",
        "$$                                                                                                                                                                                                        \n",
        "                                                                                                                                                                     "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlZ8zw9PUgQr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def attention(query, key, value, mask=None, dropout=0.0):\n",
        "    \"Compute 'Scaled Dot Product Attention'\"\n",
        "    d_k = query.size(-1)\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
        "             / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "    p_attn = F.softmax(scores, dim = -1)\n",
        "    # (Dropout described below)\n",
        "    p_attn = F.dropout(p_attn, p=dropout)\n",
        "    return torch.matmul(p_attn, value), p_attn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AV7cIqbrUgQs",
        "colab_type": "text"
      },
      "source": [
        "The two most commonly used attention functions are additive attention [(cite)](bahdanau2014neural), and dot-product (multiplicative) attention.  Dot-product attention is identical to our algorithm, except for the scaling factor of $\\frac{1}{\\sqrt{d_k}}$. Additive attention computes the compatibility function using a feed-forward network with a single hidden layer.  While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.                                                                                             \n",
        "\n",
        "                                                                        \n",
        "While for small values of $d_k$ the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of $d_k$ [(cite)](DBLP:journals/corr/BritzGLL17). We suspect that for large values of $d_k$, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients  (To illustrate why the dot products get large, assume that the components of $q$ and $k$ are independent random variables with mean $0$ and variance $1$.  Then their dot product, $q \\cdot k = \\sum_{i=1}^{d_k} q_ik_i$, has mean $0$ and variance $d_k$.). To counteract this effect, we scale the dot products by $\\frac{1}{\\sqrt{d_k}}$.          "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OV7kNMbKUgQt",
        "colab_type": "text"
      },
      "source": [
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiaCxaGGUgQt",
        "colab_type": "text"
      },
      "source": [
        "### Multi-Head Attention                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
        "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
        "Instead of performing a single attention function with $d_{\\text{model}}$-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values $h$ times with different, learned linear projections to $d_k$, $d_k$ and $d_v$ dimensions, respectively.                                                                                                                                                                                                   \n",
        "On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding $d_v$-dimensional output values. These are concatenated and once again projected, resulting in the final values:\n",
        "\n",
        "<img width=\"270px\" src=\"ModalNet-20.png\">\n",
        "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
        "Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.                                                                                                                                                                                                                                                                                             \n",
        "    \n",
        "    \n",
        "   \n",
        "$$    \n",
        "\\mathrm{MultiHead}(Q, K, V) = \\mathrm{Concat}(\\mathrm{head_1}, ..., \\mathrm{head_h})W^O    \\\\                                           \n",
        "    \\text{where}~\\mathrm{head_i} = \\mathrm{Attention}(QW^Q_i, KW^K_i, VW^V_i)                                \n",
        "$$                                                                                                                                                                                                                                                                                                                                                                         \n",
        "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
        "Where the projections are parameter matrices $W^Q_i \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}$, $W^K_i \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}$, $W^V_i \\in \\mathbb{R}^{d_{\\text{model}} \\times d_v}$ and $W^O \\in \\mathbb{R}^{hd_v \\times d_{\\text{model}}}$.                                                                                                                                                                                                                                                        \n",
        "   \n",
        "\n",
        "   \n",
        "In this work we employ $h=8$ parallel attention layers, or heads. For each of these we use $d_k=d_v=d_{\\text{model}}/h=64$. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3q9U_LaMFjMA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9b8c42d8-628c-4665-bf74-42e2a0ebe326"
      },
      "source": [
        "name = [ \"Manjeet\", \"Nikhil\", \"Shambhavi\", \"Astha\" ] \n",
        "roll_no = [ 4, 1, 3, 2,5 ] \n",
        "mapped = zip(name, roll_no) \n",
        "# converting values to print as set \n",
        "mapped = set(mapped) \n",
        "print(mapped)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{('Nikhil', 1), ('Manjeet', 4), ('Astha', 2), ('Shambhavi', 3)}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ea0UrEgUgQt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self, h, d_model, dropout=0.1):\n",
        "        \"Take in model size and number of heads.\"\n",
        "        super(MultiHeadedAttention, self).__init__()\n",
        "        assert d_model % h == 0\n",
        "        # We assume d_v always equals d_k\n",
        "        self.d_k = d_model // h\n",
        "        self.h = h\n",
        "        self.p = dropout\n",
        "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
        "        self.attn = None\n",
        "        \n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        \"Implements Figure 2\"\n",
        "        if mask is not None:\n",
        "            # Same mask applied to all h heads.\n",
        "            mask = mask.unsqueeze(1)\n",
        "        nbatches = query.size(0)\n",
        "        \n",
        "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
        "        query, key, value = [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
        "                             for l, x in zip(self.linears, (query, key, value))]\n",
        "        # self.linears[4] has not be used.\n",
        "        \n",
        "        # 2) Apply attention on all the projected vectors in batch. \n",
        "        x, self.attn = attention(query, key, value, mask=mask, dropout=self.p)\n",
        "        \n",
        "        # 3) \"Concat\" using a view and apply a final linear. \n",
        "        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)\n",
        "        return self.linears[-1](x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVdpQ_KwUgQv",
        "colab_type": "text"
      },
      "source": [
        "### Applications of Attention in our Model                                                                                                                                                      \n",
        "The Transformer uses multi-head attention in three different ways:                                                        \n",
        "1) In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder.   This allows every position in the decoder to attend over all positions in the input sequence.  This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [(cite)](wu2016google, bahdanau2014neural,JonasFaceNet2017).    \n",
        "\n",
        "\n",
        "2) The encoder contains self-attention layers.  In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder.   Each position in the encoder can attend to all positions in the previous layer of the encoder.                                                   \n",
        "\n",
        "\n",
        "3) Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position.  We need to prevent leftward information flow in the decoder to preserve the auto-regressive property.  We implement this inside of scaled dot-product attention by masking out (setting to $-\\infty$) all values in the input of the softmax which correspond to illegal connections.                                                                                                                                                                                                                                                      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gERLhK-FUgQw",
        "colab_type": "text"
      },
      "source": [
        "## Position-wise Feed-Forward Networks                                                                                                                                                                                                                                                                                                                                                             \n",
        "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
        "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically.  This consists of two linear transformations with a ReLU activation in between.\n",
        "\n",
        "$$\\mathrm{FFN}(x)=\\max(0, xW_1 + b_1) W_2 + b_2$$                                                                                                                                                                                                                                                         \n",
        "                                                                                                                                                                                                                                                        \n",
        "While the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1.  The dimensionality of input and output is $d_{\\text{model}}=512$, and the inner-layer has dimensionality $d_{ff}=2048$. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuDPthO2UgQx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "    \"Implements FFN equation.\"\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        # Torch linears have a `b` by default. \n",
        "        self.w_1 = nn.Linear(d_model, d_ff)\n",
        "        self.w_2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w_2(self.dropout(F.relu(self.w_1(x))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68VLwifsUgQz",
        "colab_type": "text"
      },
      "source": [
        "## Embeddings and Softmax                                                                                                                                                                                                                                                                                           \n",
        "Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension $d_{\\text{model}}$.  We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities.  In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [(cite)](press2016using). In the embedding layers, we multiply those weights by $\\sqrt{d_{\\text{model}}}$.                                                                                                                                 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sl5JzPeGUgQz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Embeddings(nn.Module):\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Embeddings, self).__init__()\n",
        "        self.lut = nn.Embedding(vocab, d_model)\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lut(x) * math.sqrt(self.d_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_hw5TyCUgQ1",
        "colab_type": "text"
      },
      "source": [
        "## Positional Encoding                                                                                                                             \n",
        "Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence.  To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks.  The positional encodings have the same dimension $d_{\\text{model}}$ as the embeddings, so that the two can be summed.   There are many choices of positional encodings, learned and fixed [(cite)](JonasFaceNet2017). \n",
        "\n",
        "In this work, we use sine and cosine functions of different frequencies:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
        "$$                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
        "    PE_{(pos,2i)} = sin(pos / 10000^{2i/d_{\\text{model}}}) \\\\                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
        "    PE_{(pos,2i+1)} = cos(pos / 10000^{2i/d_{\\text{model}}})                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
        "$$                                                                                                                                                                                                                                                        \n",
        "where $pos$ is the position and $i$ is the dimension.  That is, each dimension of the positional encoding corresponds to a sinusoid.  The wavelengths form a geometric progression from $2\\pi$ to $10000 \\cdot 2\\pi$.  We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset $k$, $PE_{pos+k}$ can be represented as a linear function of $PE_{pos}$. \n",
        "\n",
        "In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks.  For the base model, we use a rate of $P_{drop}=0.1$. \n",
        "                                                                                                                                                                                                                                                    \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYOt2lb7VD6J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "d475c3ef-ce46-4a52-eed0-86e9398f39ff"
      },
      "source": [
        "a=np.arange(0,12).reshape(3,2,2)\n",
        "print(a)\n",
        "print(a[:,:1])\n",
        "print(a[:,:1,:])"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[ 0  1]\n",
            "  [ 2  3]]\n",
            "\n",
            " [[ 4  5]\n",
            "  [ 6  7]]\n",
            "\n",
            " [[ 8  9]\n",
            "  [10 11]]]\n",
            "[[[0 1]]\n",
            "\n",
            " [[4 5]]\n",
            "\n",
            " [[8 9]]]\n",
            "[[[0 1]]\n",
            "\n",
            " [[4 5]]\n",
            "\n",
            " [[8 9]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVsjhp6uUgQ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"Implement the PE function.\"\n",
        "    def __init__(self, d_model, dropout, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \n",
        "        # Compute the positional encodings once in log space.\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) *\n",
        "                             -(math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x + Variable(self.pe[:, :x.size(1)], requires_grad=False)\n",
        "        return self.dropout(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMsBRCuLUgQ3",
        "colab_type": "code",
        "outputId": "8f8deea2-0eae-49c5-98be-2dff51898e6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "# The positional encoding will add in a sine wave based on position.\n",
        "# The frequency and offset of the wave is different for each dimension.\n",
        "plt.figure(figsize=(15, 5))\n",
        "pe = PositionalEncoding(20, 0)\n",
        "y = pe.forward(Variable(torch.zeros(1, 100, 20)))\n",
        "plt.plot(np.arange(100), y[0, :, 4:8].data.numpy())\n",
        "plt.legend([\"dim %d\"%p for p in [4,5,6,7]])\n",
        "None"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3wAAAEyCAYAAACh2dIXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nG5r1AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HjtmgH1UgQ5",
        "colab_type": "text"
      },
      "source": [
        "We also experimented with using learned positional embeddings [(cite)](JonasFaceNet2017) instead, and found that the two versions produced nearly identical results.  We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtnFnHH9UgQ6",
        "colab_type": "text"
      },
      "source": [
        "## Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heaKRIaZUgQ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    \"Standard generation step. (Not described in the paper.)\"\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Generator, self).__init__()\n",
        "        self.proj = nn.Linear(d_model, vocab)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.log_softmax(self.proj(x), dim=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0i97-Y7AUgQ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd3lP9fTUgQ9",
        "colab_type": "text"
      },
      "source": [
        "## Full Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-LDhRoaUgQ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_model(src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
        "    \"Construct a model object based on hyperparameters.\"\n",
        "    c = copy.deepcopy\n",
        "    attn = MultiHeadedAttention(h, d_model, dropout)\n",
        "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "    position = PositionalEncoding(d_model, dropout)\n",
        "    model = EncoderDecoder(\n",
        "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
        "        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),\n",
        "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
        "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
        "        Generator(d_model, tgt_vocab))\n",
        "    \n",
        "    # This was important from their code. Initialize parameters with Glorot or fan_avg.\n",
        "    for p in model.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform(p)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qP-g4KfhUgQ_",
        "colab_type": "code",
        "outputId": "e5d671a2-a9d4-461b-f08c-bf5b7a4ddb73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3023
        }
      },
      "source": [
        "# Small example model.\n",
        "tmp_model = make_model(10, 10, 2)\n",
        "tmp_model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EncoderDecoder(\n",
              "  (encoder): Encoder(\n",
              "    (layers): ModuleList(\n",
              "      (0): EncoderLayer(\n",
              "        (self_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512)\n",
              "            (1): Linear(in_features=512, out_features=512)\n",
              "            (2): Linear(in_features=512, out_features=512)\n",
              "            (3): Linear(in_features=512, out_features=512)\n",
              "          )\n",
              "        )\n",
              "        (feed_forward): PositionwiseFeedForward(\n",
              "          (w_1): Linear(in_features=512, out_features=2048)\n",
              "          (w_2): Linear(in_features=2048, out_features=512)\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (sublayer): ModuleList(\n",
              "          (0): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "          (1): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1): EncoderLayer(\n",
              "        (self_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512)\n",
              "            (1): Linear(in_features=512, out_features=512)\n",
              "            (2): Linear(in_features=512, out_features=512)\n",
              "            (3): Linear(in_features=512, out_features=512)\n",
              "          )\n",
              "        )\n",
              "        (feed_forward): PositionwiseFeedForward(\n",
              "          (w_1): Linear(in_features=512, out_features=2048)\n",
              "          (w_2): Linear(in_features=2048, out_features=512)\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (sublayer): ModuleList(\n",
              "          (0): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "          (1): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (norm): LayerNorm(\n",
              "    )\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (layers): ModuleList(\n",
              "      (0): DecoderLayer(\n",
              "        (self_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512)\n",
              "            (1): Linear(in_features=512, out_features=512)\n",
              "            (2): Linear(in_features=512, out_features=512)\n",
              "            (3): Linear(in_features=512, out_features=512)\n",
              "          )\n",
              "        )\n",
              "        (src_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512)\n",
              "            (1): Linear(in_features=512, out_features=512)\n",
              "            (2): Linear(in_features=512, out_features=512)\n",
              "            (3): Linear(in_features=512, out_features=512)\n",
              "          )\n",
              "        )\n",
              "        (feed_forward): PositionwiseFeedForward(\n",
              "          (w_1): Linear(in_features=512, out_features=2048)\n",
              "          (w_2): Linear(in_features=2048, out_features=512)\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (sublayer): ModuleList(\n",
              "          (0): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "          (1): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "          (2): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1): DecoderLayer(\n",
              "        (self_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512)\n",
              "            (1): Linear(in_features=512, out_features=512)\n",
              "            (2): Linear(in_features=512, out_features=512)\n",
              "            (3): Linear(in_features=512, out_features=512)\n",
              "          )\n",
              "        )\n",
              "        (src_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512)\n",
              "            (1): Linear(in_features=512, out_features=512)\n",
              "            (2): Linear(in_features=512, out_features=512)\n",
              "            (3): Linear(in_features=512, out_features=512)\n",
              "          )\n",
              "        )\n",
              "        (feed_forward): PositionwiseFeedForward(\n",
              "          (w_1): Linear(in_features=512, out_features=2048)\n",
              "          (w_2): Linear(in_features=2048, out_features=512)\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (sublayer): ModuleList(\n",
              "          (0): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "          (1): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "          (2): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (norm): LayerNorm(\n",
              "    )\n",
              "  )\n",
              "  (src_embed): Sequential(\n",
              "    (0): Embeddings(\n",
              "      (lut): Embedding(10, 512)\n",
              "    )\n",
              "    (1): PositionalEncoding(\n",
              "      (dropout): Dropout(p=0.1)\n",
              "    )\n",
              "  )\n",
              "  (tgt_embed): Sequential(\n",
              "    (0): Embeddings(\n",
              "      (lut): Embedding(10, 512)\n",
              "    )\n",
              "    (1): PositionalEncoding(\n",
              "      (dropout): Dropout(p=0.1)\n",
              "    )\n",
              "  )\n",
              "  (generator): Generator(\n",
              "    (proj): Linear(in_features=512, out_features=10)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuV1e8nEUgRB",
        "colab_type": "text"
      },
      "source": [
        "# Training\n",
        "\n",
        "This section describes the training regime for our models.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhVxtlZaUgRC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naFpt_GUUgRD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X60DPvyaUgRF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lz6n3REAUgRH",
        "colab_type": "text"
      },
      "source": [
        "## Training Data and Batching\n",
        "\n",
        "We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs.  Sentences were encoded using byte-pair encoding \\citep{DBLP:journals/corr/BritzGLL17}, which has a shared source-target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [(cite)](wu2016google). \n",
        "\n",
        "\n",
        "Sentence pairs were batched together by approximate sequence length.  Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.     "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRoJURieUgRH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52xwbbb4UgRK",
        "colab_type": "text"
      },
      "source": [
        "## Hardware and Schedule                                                                                                                                                                                                   \n",
        "We trained our models on one machine with 8 NVIDIA P100 GPUs.  For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds.  We trained the base models for a total of 100,000 steps or 12 hours.  For our big models, step time was 1.0 seconds.  The big models were trained for 300,000 steps (3.5 days)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPp__T_uUgRK",
        "colab_type": "text"
      },
      "source": [
        "## Optimizer\n",
        "\n",
        "We used the Adam optimizer [(cite)](kingma2014adam) with $\\beta_1=0.9$, $\\beta_2=0.98$ and $\\epsilon=10^{-9}$.  We varied the learning rate over the course of training, according to the formula:                                                                                            \n",
        "$$                                                                                                                                                                                                                                                                                         \n",
        "lrate = d_{\\text{model}}^{-0.5} \\cdot                                                                                                                                                                                                                                                                                                \n",
        "  \\min({step\\_num}^{-0.5},                                                                                                                                                                                                                                                                                                  \n",
        "    {step\\_num} \\cdot {warmup\\_steps}^{-1.5})                                                                                                                                                                                                                                                                               \n",
        "$$                                                                                                                                                                                             \n",
        "This corresponds to increasing the learning rate linearly for the first $warmup\\_steps$ training steps, and decreasing it thereafter proportionally to the inverse square root of the step number.  We used $warmup\\_steps=4000$.                            "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5yV0f2QUgRL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Note: This part is incredibly important. \n",
        "# Need to train with this setup of the model is very unstable.\n",
        "class NoamOpt:\n",
        "    \"Optim wrapper that implements rate.\"\n",
        "    def __init__(self, model_size, factor, warmup, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "        self._step = 0\n",
        "        self.warmup = warmup\n",
        "        self.factor = factor\n",
        "        self.model_size = model_size\n",
        "        self._rate = 0\n",
        "        \n",
        "    def step(self):\n",
        "        \"Update parameters and rate\"\n",
        "        self._step += 1\n",
        "        rate = self.rate()\n",
        "        for p in self.optimizer.param_groups:\n",
        "            p['lr'] = rate\n",
        "        self._rate = rate\n",
        "        self.optimizer.step()\n",
        "        \n",
        "    def rate(self, step = None):\n",
        "        \"Implement `lrate` above\"\n",
        "        if step is None:\n",
        "            step = self._step\n",
        "        return self.factor * \\\n",
        "            (self.model_size ** (-0.5) *\n",
        "            min(step ** (-0.5), step * self.warmup**(-1.5)))\n",
        "        \n",
        "def get_std_opt(model):\n",
        "    return NoamOpt(model.src_embed[0].d_model, 2, 4000,\n",
        "            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FM5n1PJxUgRN",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PC2wCVEFUgRO",
        "colab_type": "code",
        "outputId": "b3fc5e15-3be4-4e1b-fb3b-fd722207a9a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        }
      },
      "source": [
        "# Three settings of the lrate hyperparameters.\n",
        "opts = [NoamOpt(512, 1, 4000, None), \n",
        "        NoamOpt(512, 1, 8000, None),\n",
        "        NoamOpt(256, 1, 4000, None)]\n",
        "plt.plot(np.arange(1, 20000), [[opt.rate(i) for opt in opts] for i in range(1, 20000)])\n",
        "plt.legend([\"512:4000\", \"512:8000\", \"256:4000\"])\n",
        "None"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD4CAYAAAAO9oqkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxhhjsGRgjDEGSwbGGGOwZGCMMQb4f/cwEdYdtsdgAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f1818812828>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tkzxQYKUgRQ",
        "colab_type": "text"
      },
      "source": [
        "## Regularization                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
        "                                                                                                                                                                                                                                                                                                                      \n",
        "### Label Smoothing\n",
        "\n",
        "During training, we employed label smoothing of value $\\epsilon_{ls}=0.1$ [(cite)](DBLP:journals/corr/SzegedyVISW15).  This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVWDJLXrUgRQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LabelSmoothing(nn.Module):\n",
        "    \"Implement label smoothing.\"\n",
        "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
        "        super(LabelSmoothing, self).__init__()\n",
        "        self.criterion = nn.KLDivLoss(size_average=False)\n",
        "        self.padding_idx = padding_idx\n",
        "        self.confidence = 1.0 - smoothing\n",
        "        self.smoothing = smoothing\n",
        "        self.size = size\n",
        "        self.true_dist = None\n",
        "        \n",
        "    def forward(self, x, target):\n",
        "        assert x.size(1) == self.size\n",
        "        true_dist = x.data.clone()\n",
        "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
        "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
        "        true_dist[:, self.padding_idx] = 0\n",
        "        mask = torch.nonzero(target.data == self.padding_idx)\n",
        "        if mask.dim() > 0:\n",
        "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
        "        self.true_dist = true_dist\n",
        "        return self.criterion(x, Variable(true_dist, requires_grad=False))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gXwt5HVUgRT",
        "colab_type": "code",
        "outputId": "3ea1a660-0bc7-4b1e-ba8e-7d3fbf560640",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        }
      },
      "source": [
        "#Example\n",
        "crit = LabelSmoothing(5, 0, 0.5)\n",
        "predict = torch.FloatTensor([[0, 0.2, 0.7, 0.1, 0],\n",
        "                             [0, 0.2, 0.7, 0.1, 0], \n",
        "                             [0, 0.2, 0.7, 0.1, 0]])\n",
        "v = crit(Variable(predict.log()), \n",
        "         Variable(torch.LongTensor([2, 1, 0])))\n",
        "\n",
        "# Show the target distributions expected by the system.\n",
        "plt.imshow(crit.true_dist)\n",
        "None"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAADsCAYAAAB+Hb1HAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4Y9HCc4AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f181876c358>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHBHTwmjUgRU",
        "colab_type": "code",
        "outputId": "4086db46-2dd9-4566-804e-8e5f80641a69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        }
      },
      "source": [
        "# Label smoothing starts to penalize the model \n",
        "# if it gets very confident about a given choice\n",
        "crit = LabelSmoothing(5, 0, 0.2)\n",
        "def loss(x):\n",
        "    d = x + 3 * 1\n",
        "    predict = torch.FloatTensor([[0, x / d, 1 / d, 1 / d, 1 / d],\n",
        "                                 ])\n",
        "    #print(predict)\n",
        "    return crit(Variable(predict.log()),\n",
        "                 Variable(torch.LongTensor([1]))).data[0]\n",
        "plt.plot(np.arange(1, 100), [loss(x) for x in range(1, 100)])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f181867fac8>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD8CAYAAABq6S8VAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nU5CLiPjc/wfdnkpfFHoNSQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f18186b34a8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoyfFgLoUgRW",
        "colab_type": "text"
      },
      "source": [
        "### Memory Optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVKyONFsUgRW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_backprop(generator, criterion, out, targets, normalize):\n",
        "    \"\"\"\n",
        "    Memory optmization. Compute each timestep separately and sum grads.\n",
        "    \"\"\"\n",
        "    assert out.size(1) == targets.size(1)\n",
        "    total = 0.0\n",
        "    out_grad = []\n",
        "    for i in range(out.size(1)):\n",
        "        out_column = Variable(out[:, i].data, requires_grad=True)\n",
        "        gen = generator(out_column)\n",
        "        loss = criterion(gen, targets[:, i]) / normalize\n",
        "        total += loss.data[0]\n",
        "        loss.backward()\n",
        "        out_grad.append(out_column.grad.data.clone())\n",
        "    out_grad = torch.stack(out_grad, dim=1)\n",
        "    out.backward(gradient=out_grad)\n",
        "    return total"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiTlbMq2UgRY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_std_mask(src, tgt, pad):\n",
        "    src_mask = (src != pad).unsqueeze(-2)\n",
        "    tgt_mask = (tgt != pad).unsqueeze(-2)\n",
        "    tgt_mask = tgt_mask & Variable(subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n",
        "    return src_mask, tgt_mask"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSF9AaKJUgRZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_epoch(train_iter, model, criterion, opt, transpose=False):\n",
        "    model.train()\n",
        "    for i, batch in enumerate(train_iter):\n",
        "        src, trg, src_mask, trg_mask = \\\n",
        "            batch.src, batch.trg, batch.src_mask, batch.trg_mask\n",
        "        out = model.forward(src, trg[:, :-1], src_mask, trg_mask[:, :-1, :-1])\n",
        "        loss = loss_backprop(model.generator, criterion, out, trg[:, 1:], batch.ntokens) \n",
        "                        \n",
        "        model_opt.step()\n",
        "        model_opt.optimizer.zero_grad()\n",
        "        if i % 10 == 1:\n",
        "            print(i, loss, model_opt._rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4kFYs7nUgRb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def valid_epoch(valid_iter, model, criterion, transpose=False):\n",
        "    model.test()\n",
        "    total = 0\n",
        "    for batch in valid_iter:\n",
        "        src, trg, src_mask, trg_mask = \\\n",
        "            batch.src, batch.trg, batch.src_mask, batch.trg_mask\n",
        "        out = model.forward(src, trg[:, :-1], src_mask, trg_mask[:, :-1, :-1])\n",
        "        loss = loss_backprop(model.generator, criterion, out, trg[:, 1:], batch.ntokens) \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJo5AZasUgRd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Batch:\n",
        "    def __init__(self, src, trg, src_mask, trg_mask, ntokens):\n",
        "        self.src = src\n",
        "        self.trg = trg\n",
        "        self.src_mask = src_mask\n",
        "        self.trg_mask = trg_mask\n",
        "        self.ntokens = ntokens\n",
        "    \n",
        "def data_gen(V, batch, nbatches):\n",
        "    for i in range(nbatches):\n",
        "        data = torch.from_numpy(np.random.randint(1, V, size=(batch, 10)))\n",
        "        src = Variable(data, requires_grad=False)\n",
        "        tgt = Variable(data, requires_grad=False)\n",
        "        src_mask, tgt_mask = make_std_mask(src, tgt, 0)\n",
        "        yield Batch(src, tgt, src_mask, tgt_mask, (tgt[1:] != 0).data.sum())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrpU5b2sUgRe",
        "colab_type": "code",
        "outputId": "00871f60-2fcb-4235-c0ab-8de02a0c069c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 604
        }
      },
      "source": [
        "V = 11\n",
        "criterion = LabelSmoothing(size=V, padding_idx=0, smoothing=0.0)\n",
        "model = make_model(V, V, N=2)\n",
        "model_opt = get_std_opt(model)\n",
        "for epoch in range(2):\n",
        "    train_epoch(data_gen(V, 30, 20), model, criterion, model_opt)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 2.9646920561790466 6.987712429686844e-07\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-347a415cdfd4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel_opt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_std_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_opt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-38-1735085d52f5>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(train_iter, model, criterion, opt, transpose)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_backprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mntokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mmodel_opt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mmodel_opt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-cac760f2f089>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0mbias_correction1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5S2BcIoOUgRg",
        "colab_type": "text"
      },
      "source": [
        "# A Real World Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aP_oq0kLUgRh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For data loading.\n",
        "from torchtext import data, datasets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKynVliVXg6F",
        "colab_type": "code",
        "outputId": "f471f71a-d329-4fa9-d188-b4dcc9283234",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 989
        }
      },
      "source": [
        "!pip install torchtext spacy\n",
        "!python -m spacy download en\n",
        "!python -m spacy download de"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.6/dist-packages\r\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext)\n",
            "Requirement already satisfied: preshed<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from spacy)\n",
            "Requirement already satisfied: murmurhash<0.29,>=0.28 in /usr/local/lib/python3.6/dist-packages (from spacy)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from spacy)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy)\n",
            "Requirement already satisfied: cymem<1.32,>=1.30 in /usr/local/lib/python3.6/dist-packages (from spacy)\n",
            "Requirement already satisfied: msgpack-python==0.5.4 in /usr/local/lib/python3.6/dist-packages (from spacy)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from spacy)\n",
            "Requirement already satisfied: html5lib==1.0b8 in /usr/local/lib/python3.6/dist-packages (from spacy)\n",
            "Requirement already satisfied: thinc<6.11.0,>=6.10.1 in /usr/local/lib/python3.6/dist-packages (from spacy)\n",
            "Requirement already satisfied: pathlib in /usr/local/lib/python3.6/dist-packages (from spacy)\n",
            "Requirement already satisfied: ujson>=1.35 in /usr/local/lib/python3.6/dist-packages (from spacy)\n",
            "Requirement already satisfied: dill<0.3,>=0.2 in /usr/local/lib/python3.6/dist-packages (from spacy)\n",
            "Requirement already satisfied: msgpack-numpy==0.4.1 in /usr/local/lib/python3.6/dist-packages (from spacy)\n",
            "Requirement already satisfied: ftfy<5.0.0,>=4.4.2 in /usr/local/lib/python3.6/dist-packages (from spacy)\n",
            "Requirement already satisfied: regex==2017.4.5 in /usr/local/lib/python3.6/dist-packages (from spacy)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext)\n",
            "Requirement already satisfied: cytoolz<0.9,>=0.8 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.1->spacy)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.1->spacy)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.1->spacy)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy<5.0.0,>=4.4.2->spacy)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz<0.9,>=0.8->thinc<6.11.0,>=6.10.1->spacy)\n",
            "Collecting https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz (37.4MB)\n",
            "\u001b[K    6% |██                              | 2.4MB 47.2MB/s eta 0:00:01"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[K    100% |████████████████████████████████| 37.4MB 50.2MB/s \n",
            "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
            "  Running setup.py install for en-core-web-sm ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n",
            "\u001b[?25hSuccessfully installed en-core-web-sm-2.0.0\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "    /usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "    /usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "\n",
            "    You can now load the model via spacy.load('en')\n",
            "\n",
            "Collecting https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.0.0/de_core_news_sm-2.0.0.tar.gz\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.0.0/de_core_news_sm-2.0.0.tar.gz (38.2MB)\n",
            "\u001b[K    100% |████████████████████████████████| 38.2MB 50.0MB/s \n",
            "\u001b[?25hInstalling collected packages: de-core-news-sm\n",
            "  Running setup.py install for de-core-news-sm ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n",
            "\u001b[?25hSuccessfully installed de-core-news-sm-2.0.0\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "    /usr/local/lib/python3.6/dist-packages/de_core_news_sm -->\n",
            "    /usr/local/lib/python3.6/dist-packages/spacy/data/de\n",
            "\n",
            "    You can now load the model via spacy.load('de')\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXtYwdHqUgRj",
        "colab_type": "code",
        "outputId": "31ee7819-a5eb-4cf2-cb06-cc7932bf535a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        }
      },
      "source": [
        "# Load words from IWSLT\n",
        "\n",
        "#!pip install torchtext spacy\n",
        "#!python -m spacy download en\n",
        "#!python -m spacy download de\n",
        "\n",
        "import spacy\n",
        "spacy_de = spacy.load('de')\n",
        "spacy_en = spacy.load('en')\n",
        "\n",
        "def tokenize_de(text):\n",
        "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
        "\n",
        "def tokenize_en(text):\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
        "\n",
        "BOS_WORD = '<s>'\n",
        "EOS_WORD = '</s>'\n",
        "BLANK_WORD = \"<blank>\"\n",
        "SRC = data.Field(tokenize=tokenize_de, pad_token=BLANK_WORD)\n",
        "TGT = data.Field(tokenize=tokenize_en, init_token = BOS_WORD, \n",
        "                 eos_token = EOS_WORD, pad_token=BLANK_WORD)\n",
        "\n",
        "MAX_LEN = 100\n",
        "train, val, test = datasets.IWSLT.splits(exts=('.de', '.en'), fields=(SRC, TGT), \n",
        "                                         filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and \n",
        "                                         len(vars(x)['trg']) <= MAX_LEN)\n",
        "MIN_FREQ = 1\n",
        "SRC.build_vocab(train.src, min_freq=MIN_FREQ)\n",
        "TGT.build_vocab(train.trg, min_freq=MIN_FREQ)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading de-en.tgz\n",
            ".data/iwslt/de-en/IWSLT16.TEDX.dev2012.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2013.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2014.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2012.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2010.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.dev2010.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.dev2010.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TEDX.tst2013.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TEDX.dev2012.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2013.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TEDX.tst2013.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TEDX.tst2014.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2014.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2011.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2011.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2010.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TEDX.tst2014.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2012.de-en.de.xml\n",
            ".data/iwslt/de-en/train.tags.de-en.en\n",
            ".data/iwslt/de-en/train.tags.de-en.de\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8MTIJTWUgRl",
        "colab_type": "code",
        "outputId": "bd6c163a-14a2-4f8c-b0d5-01a033f278c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        }
      },
      "source": [
        "# Detail. Batching seems to matter quite a bit. \n",
        "# This is temporary code for dynamic batching based on number of tokens.\n",
        "# This code should all go away once things get merged in this library.\n",
        "\n",
        "BATCH_SIZE = 4096\n",
        "global max_src_in_batch, max_tgt_in_batch\n",
        "def batch_size_fn(new, count, sofar):\n",
        "    \"Keep augmenting batch and calculate total number of tokens + padding.\"\n",
        "    global max_src_in_batch, max_tgt_in_batch\n",
        "    if count == 1:\n",
        "        max_src_in_batch = 0\n",
        "        max_tgt_in_batch = 0\n",
        "    max_src_in_batch = max(max_src_in_batch,  len(new.src))\n",
        "    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.trg) + 2)\n",
        "    src_elements = count * max_src_in_batch\n",
        "    tgt_elements = count * max_tgt_in_batch\n",
        "    return max(src_elements, tgt_elements)\n",
        "\n",
        "class MyIterator(data.Iterator):\n",
        "    def create_batches(self):\n",
        "        if self.train:\n",
        "            def pool(d, random_shuffler):\n",
        "                for p in data.batch(d, self.batch_size * 100):\n",
        "                    p_batch = data.batch(\n",
        "                        sorted(p, key=self.sort_key),\n",
        "                        self.batch_size, self.batch_size_fn)\n",
        "                    for b in random_shuffler(list(p_batch)):\n",
        "                        yield b\n",
        "            self.batches = pool(self.data(), self.random_shuffler)\n",
        "            \n",
        "        else:\n",
        "            self.batches = []\n",
        "            for b in data.batch(self.data(), self.batch_size,\n",
        "                                          self.batch_size_fn):\n",
        "                self.batches.append(sorted(b, key=self.sort_key))\n",
        "\n",
        "def rebatch(pad_idx, batch):\n",
        "    \"Fix order in torchtext to match ours\"\n",
        "    src, trg = batch.src.transpose(0, 1), batch.trg.transpose(0, 1)\n",
        "    src_mask, trg_mask = make_std_mask(src, trg, pad_idx)\n",
        "    return Batch(src, trg, src_mask, trg_mask, (trg[1:] != pad_idx).data.sum())\n",
        "\n",
        "train_iter = MyIterator(train, batch_size=BATCH_SIZE, device=0,\n",
        "                        repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n",
        "                        batch_size_fn=batch_size_fn, train=True)\n",
        "valid_iter = MyIterator(val, batch_size=BATCH_SIZE, device=0,\n",
        "                        repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n",
        "                        batch_size_fn=batch_size_fn, train=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a98b1f496512>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_elements\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_elements\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mMyIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wamR3SPdUgRo",
        "colab_type": "code",
        "outputId": "b3b92f29-2560-4dbb-c2b0-f2a0b71db37c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1900
        }
      },
      "source": [
        "# Create the model an load it onto our GPU.\n",
        "pad_idx = TGT.vocab.stoi[\"<blank>\"]\n",
        "model = make_model(len(SRC.vocab), len(TGT.vocab), N=6)\n",
        "model_opt = get_std_opt(model)\n",
        "model.cuda()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-b72405416f0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSRC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTGT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel_opt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_std_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \"\"\"\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    150\u001b[0m                 \u001b[0;31m# Variables stored in modules are graph leaves, and we don't\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m                 \u001b[0;31m# want to create copy nodes, so we have to unpack the data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m                 \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m                     \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \"\"\"\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_cuda\u001b[0;34m(self, device, async)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mnew_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_new\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_lazy_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m     \u001b[0m_lazy_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m     \u001b[0;31m# We need this method only for lazy init, so we can remove it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0m_CudaBase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    118\u001b[0m         raise RuntimeError(\n\u001b[1;32m    119\u001b[0m             \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[0;32m--> 120\u001b[0;31m     \u001b[0m_check_driver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_sparse_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_check_driver\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0mFound\u001b[0m \u001b[0mno\u001b[0m \u001b[0mNVIDIA\u001b[0m \u001b[0mdriver\u001b[0m \u001b[0mon\u001b[0m \u001b[0myour\u001b[0m \u001b[0msystem\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mPlease\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0mthat\u001b[0m \u001b[0myou\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0mhave\u001b[0m \u001b[0man\u001b[0m \u001b[0mNVIDIA\u001b[0m \u001b[0mGPU\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minstalled\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdriver\u001b[0m \u001b[0;32mfrom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m http://www.nvidia.com/Download/index.aspx\"\"\")\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;31m# TODO: directly link to the alternative bin that needs install\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: \nFound no NVIDIA driver on your system. Please check that you\nhave an NVIDIA GPU and installed a driver from\nhttp://www.nvidia.com/Download/index.aspx"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SStCCZoiUgRp",
        "colab_type": "code",
        "outputId": "a551e444-2fe5-4cd5-f46a-3de3505a2545",
        "colab": {}
      },
      "source": [
        "\n",
        "criterion = LabelSmoothing(size=len(TGT.vocab), padding_idx=pad_idx, smoothing=0.1)\n",
        "criterion.cuda()\n",
        "for epoch in range(15):\n",
        "    train_epoch((rebatch(pad_idx, b) for b in train_iter), model, criterion, model_opt)\n",
        "    valid_epoch((rebatch(pad_idx, b) for b in valid_iter), model, criterion)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 9.299771845340729 6.987712429686844e-07\n",
            "11 9.415135336574167 4.192627457812107e-06\n",
            "21 8.813630282878876 7.686483672655528e-06\n",
            "31 9.112178653478622 1.118033988749895e-05\n",
            "41 8.607461810112 1.4674196102342371e-05\n",
            "51 8.913826749660075 1.8168052317185794e-05\n",
            "61 8.701497752219439 2.1661908532029216e-05\n",
            "71 8.373274087905884 2.515576474687264e-05\n",
            "81 8.454237446188927 2.8649620961716057e-05\n",
            "91 7.6996782422065735 3.214347717655948e-05\n",
            "101 8.037408232688904 3.56373333914029e-05\n",
            "111 7.704962134361267 3.913118960624633e-05\n",
            "121 7.699015600606799 4.262504582108975e-05\n",
            "131 7.367554426193237 4.611890203593317e-05\n",
            "141 7.2071177661418915 4.961275825077659e-05\n",
            "151 7.106400920893066 5.310661446562001e-05\n",
            "161 6.804656069725752 5.660047068046343e-05\n",
            "171 6.390337720513344 6.0094326895306855e-05\n",
            "181 5.687528342008591 6.358818311015028e-05\n",
            "191 6.122820109128952 6.70820393249937e-05\n",
            "201 5.829070374369621 7.057589553983712e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NO9lsw2UgRt",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "OTHER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8BVm-hEUgRw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BOS_WORD = '<s>'\n",
        "EOS_WORD = '</s>'\n",
        "BLANK_WORD = \"<blank>\"\n",
        "SRC = data.Field()\n",
        "TGT = data.Field(init_token = BOS_WORD, eos_token = EOS_WORD, pad_token=BLANK_WORD) # only target needs BOS/EOS\n",
        "\n",
        "MAX_LEN = 100\n",
        "train = datasets.TranslationDataset(path=\"/n/home00/srush/Data/baseline-1M_train.tok.shuf\", \n",
        "                                    exts=('.en', '.fr'),\n",
        "                                    fields=(SRC, TGT), \n",
        "                                    filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and \n",
        "                                         len(vars(x)['trg']) <= MAX_LEN)\n",
        "SRC.build_vocab(train.src, max_size=50000)\n",
        "TGT.build_vocab(train.trg, max_size=50000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFdZyOIzUgRx",
        "colab_type": "code",
        "outputId": "bc543dba-c343-47bc-e81e-f74a25688668",
        "colab": {}
      },
      "source": [
        "pad_idx = TGT.vocab.stoi[\"<blank>\"]\n",
        "print(pad_idx)\n",
        "model = make_model(len(SRC.vocab), len(TGT.vocab), pad_idx, N=6)\n",
        "model_opt = get_opt(model)\n",
        "model.cuda()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EncoderDecoder(\n",
              "  (encoder): Encoder(\n",
              "    (layers): ModuleList(\n",
              "      (0): EncoderLayer(\n",
              "        (self_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512)\n",
              "            (1): Linear(in_features=512, out_features=512)\n",
              "            (2): Linear(in_features=512, out_features=512)\n",
              "            (3): Linear(in_features=512, out_features=512)\n",
              "          )\n",
              "        )\n",
              "        (feed_forward): PositionwiseFeedForward(\n",
              "          (w_1): Linear(in_features=512, out_features=2048)\n",
              "          (w_2): Linear(in_features=2048, out_features=512)\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (sublayer): ModuleList(\n",
              "          (0): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "          (1): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1): EncoderLayer(\n",
              "        (self_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512)\n",
              "            (1): Linear(in_features=512, out_features=512)\n",
              "            (2): Linear(in_features=512, out_features=512)\n",
              "            (3): Linear(in_features=512, out_features=512)\n",
              "          )\n",
              "        )\n",
              "        (feed_forward): PositionwiseFeedForward(\n",
              "          (w_1): Linear(in_features=512, out_features=2048)\n",
              "          (w_2): Linear(in_features=2048, out_features=512)\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (sublayer): ModuleList(\n",
              "          (0): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "          (1): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (2): EncoderLayer(\n",
              "        (self_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512)\n",
              "            (1): Linear(in_features=512, out_features=512)\n",
              "            (2): Linear(in_features=512, out_features=512)\n",
              "            (3): Linear(in_features=512, out_features=512)\n",
              "          )\n",
              "        )\n",
              "        (feed_forward): PositionwiseFeedForward(\n",
              "          (w_1): Linear(in_features=512, out_features=2048)\n",
              "          (w_2): Linear(in_features=2048, out_features=512)\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (sublayer): ModuleList(\n",
              "          (0): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "          (1): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (3): EncoderLayer(\n",
              "        (self_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512)\n",
              "            (1): Linear(in_features=512, out_features=512)\n",
              "            (2): Linear(in_features=512, out_features=512)\n",
              "            (3): Linear(in_features=512, out_features=512)\n",
              "          )\n",
              "        )\n",
              "        (feed_forward): PositionwiseFeedForward(\n",
              "          (w_1): Linear(in_features=512, out_features=2048)\n",
              "          (w_2): Linear(in_features=2048, out_features=512)\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (sublayer): ModuleList(\n",
              "          (0): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "          (1): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (4): EncoderLayer(\n",
              "        (self_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512)\n",
              "            (1): Linear(in_features=512, out_features=512)\n",
              "            (2): Linear(in_features=512, out_features=512)\n",
              "            (3): Linear(in_features=512, out_features=512)\n",
              "          )\n",
              "        )\n",
              "        (feed_forward): PositionwiseFeedForward(\n",
              "          (w_1): Linear(in_features=512, out_features=2048)\n",
              "          (w_2): Linear(in_features=2048, out_features=512)\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (sublayer): ModuleList(\n",
              "          (0): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "          (1): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (5): EncoderLayer(\n",
              "        (self_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512)\n",
              "            (1): Linear(in_features=512, out_features=512)\n",
              "            (2): Linear(in_features=512, out_features=512)\n",
              "            (3): Linear(in_features=512, out_features=512)\n",
              "          )\n",
              "        )\n",
              "        (feed_forward): PositionwiseFeedForward(\n",
              "          (w_1): Linear(in_features=512, out_features=2048)\n",
              "          (w_2): Linear(in_features=2048, out_features=512)\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (sublayer): ModuleList(\n",
              "          (0): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "          (1): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (norm): LayerNorm(\n",
              "    )\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (layers): ModuleList(\n",
              "      (0): DecoderLayer(\n",
              "        (self_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512)\n",
              "            (1): Linear(in_features=512, out_features=512)\n",
              "            (2): Linear(in_features=512, out_features=512)\n",
              "            (3): Linear(in_features=512, out_features=512)\n",
              "          )\n",
              "        )\n",
              "        (src_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512)\n",
              "            (1): Linear(in_features=512, out_features=512)\n",
              "            (2): Linear(in_features=512, out_features=512)\n",
              "            (3): Linear(in_features=512, out_features=512)\n",
              "          )\n",
              "        )\n",
              "        (feed_forward): PositionwiseFeedForward(\n",
              "          (w_1): Linear(in_features=512, out_features=2048)\n",
              "          (w_2): Linear(in_features=2048, out_features=512)\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (sublayer): ModuleList(\n",
              "          (0): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "          (1): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "          (2): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1): DecoderLayer(\n",
              "        (self_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512)\n",
              "            (1): Linear(in_features=512, out_features=512)\n",
              "            (2): Linear(in_features=512, out_features=512)\n",
              "            (3): Linear(in_features=512, out_features=512)\n",
              "          )\n",
              "        )\n",
              "        (src_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512)\n",
              "            (1): Linear(in_features=512, out_features=512)\n",
              "            (2): Linear(in_features=512, out_features=512)\n",
              "            (3): Linear(in_features=512, out_features=512)\n",
              "          )\n",
              "        )\n",
              "        (feed_forward): PositionwiseFeedForward(\n",
              "          (w_1): Linear(in_features=512, out_features=2048)\n",
              "          (w_2): Linear(in_features=2048, out_features=512)\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (sublayer): ModuleList(\n",
              "          (0): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "          (1): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "          (2): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (2): DecoderLayer(\n",
              "        (self_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512)\n",
              "            (1): Linear(in_features=512, out_features=512)\n",
              "            (2): Linear(in_features=512, out_features=512)\n",
              "            (3): Linear(in_features=512, out_features=512)\n",
              "          )\n",
              "        )\n",
              "        (src_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512)\n",
              "            (1): Linear(in_features=512, out_features=512)\n",
              "            (2): Linear(in_features=512, out_features=512)\n",
              "            (3): Linear(in_features=512, out_features=512)\n",
              "          )\n",
              "        )\n",
              "        (feed_forward): PositionwiseFeedForward(\n",
              "          (w_1): Linear(in_features=512, out_features=2048)\n",
              "          (w_2): Linear(in_features=2048, out_features=512)\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (sublayer): ModuleList(\n",
              "          (0): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "          (1): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "          (2): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (3): DecoderLayer(\n",
              "        (self_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512)\n",
              "            (1): Linear(in_features=512, out_features=512)\n",
              "            (2): Linear(in_features=512, out_features=512)\n",
              "            (3): Linear(in_features=512, out_features=512)\n",
              "          )\n",
              "        )\n",
              "        (src_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512)\n",
              "            (1): Linear(in_features=512, out_features=512)\n",
              "            (2): Linear(in_features=512, out_features=512)\n",
              "            (3): Linear(in_features=512, out_features=512)\n",
              "          )\n",
              "        )\n",
              "        (feed_forward): PositionwiseFeedForward(\n",
              "          (w_1): Linear(in_features=512, out_features=2048)\n",
              "          (w_2): Linear(in_features=2048, out_features=512)\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (sublayer): ModuleList(\n",
              "          (0): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "          (1): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "          (2): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (4): DecoderLayer(\n",
              "        (self_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512)\n",
              "            (1): Linear(in_features=512, out_features=512)\n",
              "            (2): Linear(in_features=512, out_features=512)\n",
              "            (3): Linear(in_features=512, out_features=512)\n",
              "          )\n",
              "        )\n",
              "        (src_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512)\n",
              "            (1): Linear(in_features=512, out_features=512)\n",
              "            (2): Linear(in_features=512, out_features=512)\n",
              "            (3): Linear(in_features=512, out_features=512)\n",
              "          )\n",
              "        )\n",
              "        (feed_forward): PositionwiseFeedForward(\n",
              "          (w_1): Linear(in_features=512, out_features=2048)\n",
              "          (w_2): Linear(in_features=2048, out_features=512)\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (sublayer): ModuleList(\n",
              "          (0): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "          (1): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "          (2): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (5): DecoderLayer(\n",
              "        (self_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512)\n",
              "            (1): Linear(in_features=512, out_features=512)\n",
              "            (2): Linear(in_features=512, out_features=512)\n",
              "            (3): Linear(in_features=512, out_features=512)\n",
              "          )\n",
              "        )\n",
              "        (src_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512)\n",
              "            (1): Linear(in_features=512, out_features=512)\n",
              "            (2): Linear(in_features=512, out_features=512)\n",
              "            (3): Linear(in_features=512, out_features=512)\n",
              "          )\n",
              "        )\n",
              "        (feed_forward): PositionwiseFeedForward(\n",
              "          (w_1): Linear(in_features=512, out_features=2048)\n",
              "          (w_2): Linear(in_features=2048, out_features=512)\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (sublayer): ModuleList(\n",
              "          (0): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "          (1): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "          (2): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (norm): LayerNorm(\n",
              "    )\n",
              "  )\n",
              "  (src_embed): Sequential(\n",
              "    (0): Embeddings(\n",
              "      (lut): Embedding(50002, 512)\n",
              "    )\n",
              "    (1): PositionalEncoding(\n",
              "      (dropout): Dropout(p=0.1)\n",
              "    )\n",
              "  )\n",
              "  (tgt_embed): Sequential(\n",
              "    (0): Embeddings(\n",
              "      (lut): Embedding(50004, 512)\n",
              "    )\n",
              "    (1): PositionalEncoding(\n",
              "      (dropout): Dropout(p=0.1)\n",
              "    )\n",
              "  )\n",
              "  (generator): Generator(\n",
              "    (proj): Linear(in_features=512, out_features=50004)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cinuTkbtUgRz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = LabelSmoothing(size=len(TGT.vocab), padding_idx=pad_idx, label_smoothing=0.1)\n",
        "criterion.cuda()\n",
        "for epoch in range(15):\n",
        "    train_epoch(train_iter, model, criterion, model_opt)\n",
        "    valid_epoch()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsoeJn4bUgR1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LadFBIEUgR3",
        "colab_type": "code",
        "outputId": "ba9a812f-b5f6-4972-af91-215d91a223d1",
        "colab": {}
      },
      "source": [
        "print(pad_idx)\n",
        "print(len(SRC.vocab))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "50002\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kP_Au0bHUgR7",
        "colab_type": "code",
        "outputId": "d5ad5d88-d512-4947-b5b8-f18eaba4f9ff",
        "colab": {}
      },
      "source": [
        "torch.save(model, \"/n/rush_lab/trans_ipython.pt\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/n/home00/srush/.conda/envs/py3/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type EncoderDecoder. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/n/home00/srush/.conda/envs/py3/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type Encoder. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/n/home00/srush/.conda/envs/py3/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type EncoderLayer. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/n/home00/srush/.conda/envs/py3/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type MultiHeadedAttention. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/n/home00/srush/.conda/envs/py3/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type PositionwiseFeedForward. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/n/home00/srush/.conda/envs/py3/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type SublayerConnection. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/n/home00/srush/.conda/envs/py3/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type LayerNorm. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/n/home00/srush/.conda/envs/py3/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type Decoder. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/n/home00/srush/.conda/envs/py3/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type DecoderLayer. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/n/home00/srush/.conda/envs/py3/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type Embeddings. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/n/home00/srush/.conda/envs/py3/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type PositionalEncoding. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/n/home00/srush/.conda/envs/py3/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type Generator. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqKKIhoOUgR-",
        "colab_type": "code",
        "outputId": "2c087978-9803-4639-886b-88df91e62096",
        "colab": {}
      },
      "source": [
        "#weight = torch.ones(len(TGT.vocab))\n",
        "#weight[pad_idx] = 0\n",
        "#criterion = nn.NLLLoss(size_average=False, weight=weight.cuda())\n",
        "criterion = LabelSmoothing(size=len(TGT.vocab), padding_idx=pad_idx, label_smoothing=0.1)\n",
        "criterion.cuda()\n",
        "for epoch in range(15):\n",
        "    train_epoch(train_iter, model, criterion, model_opt)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 3.269582842476666 0.0005377044714644026\n",
            "101 3.300532897672383 0.0005726430336128369\n",
            "201 3.3047672072425485 0.0006075815957612711\n",
            "301 2.7151080842595547 0.0006425201579097052\n",
            "401 2.6975380268413574 0.0006774587200581396\n",
            "501 3.051631387323141 0.0007123972822065737\n",
            "601 2.554425454698503 0.000747335844355008\n",
            "701 2.6254820519825444 0.0007822744065034422\n",
            "801 2.868743653933052 0.0008172129686518764\n",
            "901 2.5978208642918617 0.0008521515308003106\n",
            "1001 2.5955790174775757 0.0008870900929487448\n",
            "1101 2.6764775353949517 0.000922028655097179\n",
            "1201 2.464000296778977 0.0009569672172456132\n",
            "1301 2.0503073083236814 0.0009919057793940475\n",
            "1401 2.295472824771423 0.0010268443415424816\n",
            "1501 2.245281406212598 0.0010617829036909158\n",
            "1601 2.2577588511630893 0.00109672146583935\n",
            "1701 2.2232908592559397 0.0011316600279877844\n",
            "1801 2.357596361427568 0.0011665985901362186\n",
            "1901 2.121352154412307 0.0012015371522846527\n",
            "2001 2.5742998471250758 0.001236475714433087\n",
            "2101 2.2518509055953473 0.0012714142765815214\n",
            "2201 2.2251326659170445 0.0013063528387299553\n",
            "2301 2.078994876006618 0.0013412914008783896\n",
            "2401 2.068276036065072 0.001376229963026824\n",
            "2501 2.31435151558253 0.0013907788851585368\n",
            "2601 1.9106871648691595 0.0013738752565588634\n",
            "2701 2.183084836578928 0.0013575733592730722\n",
            "2801 2.4668076275847852 0.0013418383196400342\n",
            "2901 1.963176985620521 0.0013266380295186675\n",
            "3001 2.2140520309330896 0.0013119428705609764\n",
            "3101 2.6989458349489723 0.0012977254713568687\n",
            "3201 2.1293521663174033 0.0012839604929174666\n",
            "3301 2.1402786187827587 0.0012706244386700126\n",
            "3401 2.041781216394156 0.0012576954857216498\n",
            "3501 2.051893091876991 0.0012451533346344698\n",
            "3601 1.5498304846696556 0.001232979075358713\n",
            "3701 2.763939742697403 0.001221155067309524\n",
            "3801 2.7611468499198963 0.0012096648318570434\n",
            "3901 1.7321470333263278 0.0011984929557393293\n",
            "4001 2.139603299088776 0.0011876250041103701\n",
            "4101 2.1966493157087825 0.0011770474421074978\n",
            "4201 2.0962203710805625 0.0011667475639689723\n",
            "4301 1.9717675620922819 0.0011567134288575545\n",
            "4401 2.097687987901736 0.0011469338026529508\n",
            "4501 1.9319786678534001 0.001137398105067946\n",
            "4601 1.8846281475271098 0.0011280963615221983\n",
            "4701 1.9817245414596982 0.0011190191592759865\n",
            "4801 1.7659185670199804 0.0011101576073853326\n",
            "4901 2.188665813198895 0.0011015033000912066\n",
            "5001 2.1391192222399695 0.0010930482833001135\n",
            "5101 1.8125874139368534 0.0010847850238522342\n",
            "5201 1.6616800595074892 0.0010767063813072288\n",
            "5301 1.6544548005331308 0.0010688055820075176\n",
            "5401 1.9542939933016896 0.0010610761952049212\n",
            "5501 2.218412609123334 0.0010535121110594244\n",
            "5601 1.838119359650591 0.001046107520339004\n",
            "5701 1.892627771012485 0.0010388568956672375\n",
            "5801 2.2462481096954434 0.0010317549741811346\n",
            "5901 1.4471426841337234 0.0010247967414755423\n",
            "6001 1.9312338004237972 0.0010179774167228303\n",
            "6101 1.7303275546291843 0.001011292438867507\n",
            "6201 1.8833909621462226 0.0010047374538051973\n",
            "6301 1.8943474531406537 0.0009983083024640838\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/n/home00/srush/.conda/envs/py3/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: generator 'Iterator.__iter__' raised StopIteration\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1 1.5940000533591956 0.0009927515780513657\n",
            "101 1.7524283765815198 0.0009865483707369156\n",
            "201 1.900138527940726 0.0009804600111146078\n",
            "301 1.8419977760640904 0.0009744829985071481\n",
            "401 1.9621913449373096 0.0009686139798247046\n",
            "501 2.226916428655386 0.0009628497416600543\n",
            "601 1.7190162097394932 0.0009571872028951208\n",
            "701 1.8589106332874508 0.0009516234077802563\n",
            "801 1.8107321247807704 0.000946155519450957\n",
            "901 1.6531266793608665 0.0009407808138497059\n",
            "1001 1.4840005157748237 0.0009354966740233614\n",
            "1101 1.7578403616789728 0.0009303005847689719\n",
            "1201 1.3920216620899737 0.0009251901276031373\n",
            "1301 1.6626927084289491 0.0009201629760320567\n",
            "1401 1.7256765578058548 0.0009152168911012566\n",
            "1501 1.6049046433763579 0.0009103497172056578\n",
            "1601 1.6955451717367396 0.000905559378142174\n",
            "1701 1.6796367820352316 0.0009008438733884249\n",
            "1801 1.5794002648835885 0.0008962012745924116\n",
            "1901 1.9637197174597532 0.0008916297222591652\n",
            "2001 1.4656428614398465 0.0008871274226214399\n",
            "2101 1.567156056407839 0.0008826926446824871\n",
            "2201 1.542241255287081 0.0008783237174198395\n",
            "2301 1.690121710913445 0.0008740190271398465\n",
            "2401 1.357302049640566 0.0008697770149734477\n",
            "2501 1.9049871656461619 0.0008655961745043597\n",
            "2601 2.240402895025909 0.0008614750495214811\n",
            "2701 1.7940634173137369 0.0008574122318878972\n",
            "2801 1.7314323161263019 0.0008534063595194054\n",
            "2901 1.6064868164248765 0.0008494561144659686\n",
            "3001 1.7515187719254754 0.0008455602210899614\n",
            "3101 1.552100334316492 0.0008417174443354889\n",
            "3201 1.6221882179379463 0.0008379265880834463\n",
            "3301 1.5139061958470847 0.0008341864935873445\n",
            "3401 1.6668659402348567 0.0008304960379852562\n",
            "3501 2.1993618682026863 0.0008268541328835436\n",
            "3601 1.823760490231507 0.0008232597230083089\n",
            "3701 1.8189842144493014 0.0008197117849207771\n",
            "3801 1.689056838164106 0.0008162093257930558\n",
            "3901 1.5656833801185712 0.0008127513822409492\n",
            "4001 1.5621904337021988 0.0008093370192107105\n",
            "4101 1.4836799805052578 0.0008059653289168093\n",
            "4201 1.47899504378438 0.000802635429827976\n",
            "4301 1.6922758186701685 0.0007993464656989501\n",
            "4401 1.636858390578709 0.000796097604645519\n",
            "4501 1.5558803144613194 0.0007928880382605766\n",
            "4601 1.5102424336364493 0.00078971698076907\n",
            "4701 1.541241532890126 0.0007865836682198282\n",
            "4801 1.5931309935403988 0.000783487357712386\n",
            "4901 1.2315586884506047 0.0007804273266570247\n",
            "5001 1.527937745093368 0.0007774028720663579\n",
            "5101 1.31743333209306 0.0007744133098768835\n",
            "5201 1.5960889644484269 0.0007714579742990187\n",
            "5301 1.4181096099782735 0.0007685362171942096\n",
            "5401 1.4596448407392018 0.0007656474074777987\n",
            "5501 1.4594163084111642 0.0007627909305463981\n",
            "5601 1.62109798315214 0.0007599661877285873\n",
            "5701 1.586864550015889 0.0007571725957578231\n",
            "5801 1.5062829439411871 0.0007544095862665088\n",
            "5901 1.4292167258172412 0.0007516766053002225\n",
            "6001 1.4355267270930199 0.0007489731128511653\n",
            "6101 1.4162533966591582 0.0007462985824099354\n",
            "6201 1.6518787188415445 0.0007436525005347853\n",
            "6301 1.5916137372114463 0.0007410343664375577\n",
            "1 1.202994157327339 0.0007387531385993765\n",
            "101 1.4649722938484047 0.0007361862332058686\n",
            "201 1.1459896704182029 0.0007336459004644837\n",
            "301 1.417104929103516 0.0007311316850490442\n",
            "401 1.373963651509257 0.0007286431424819469\n",
            "501 1.6432027550181374 0.0007261798388040814\n",
            "601 1.4122836171882227 0.0007237413502569408\n",
            "701 1.6119428309611976 0.0007213272629763972\n",
            "801 1.5545603609643877 0.0007189371726976359\n",
            "901 1.5427279596333392 0.0007165706844707772\n",
            "1001 1.5437391183004365 0.0007142274123867243\n",
            "1101 1.9743895339342998 0.0007119069793128112\n",
            "1201 1.730805973522365 0.0007096090166378355\n",
            "1301 1.5635135210759472 0.0007073331640260875\n",
            "1401 1.206731209764257 0.000705079069180001\n",
            "1501 1.4495476994197816 0.0007028463876110714\n",
            "1601 1.2935033895773813 0.0007006347824187037\n",
            "1701 1.1734203454107046 0.000698443924076667\n",
            "1801 1.202259551268071 0.0006962734902268488\n",
            "1901 1.7874216835407424 0.0006941231654800159\n",
            "2001 1.5438914835685864 0.0006919926412233024\n",
            "2101 1.5168145569041371 0.000689881615434157\n",
            "2201 1.5306344364071265 0.0006877897925004977\n",
            "2301 1.5227781175635755 0.0006857168830468271\n",
            "2401 1.3308223116910085 0.0006836626037660786\n",
            "2501 1.4871021673316136 0.0006816266772569715\n",
            "2601 1.3415705130901188 0.0006796088318666611\n",
            "2701 1.2746119699440897 0.0006776088015384847\n",
            "2801 1.3439618053671438 0.0006756263256646049\n",
            "2901 1.3065503026737133 0.0006736611489433701\n",
            "3001 1.4918707825127058 0.0006717130212412112\n",
            "3101 1.4003087060991675 0.0006697816974589058\n",
            "3201 1.3473156996478792 0.0006678669374020495\n",
            "3301 1.3869949235959211 0.0006659685056555759\n",
            "3401 1.5086751837225165 0.000664086171462178\n",
            "3501 1.4735991460911464 0.00066221970860449\n",
            "3601 1.3997832712557283 0.0006603688952908887\n",
            "3701 1.5196008981074556 0.0006585335140447885\n",
            "3801 1.2834229312138632 0.0006567133515973014\n",
            "3901 1.3874705795169575 0.0006549081987831418\n",
            "4001 1.6422591609880328 0.0006531178504396635\n",
            "4101 1.305389653716702 0.0006513421053089143\n",
            "4201 1.5159487561322749 0.0006495807659426053\n",
            "4301 1.3981374967552256 0.0006478336386098913\n",
            "4401 1.7390631912276149 0.0006461005332078655\n",
            "4501 1.3604947600979358 0.0006443812631746732\n",
            "4601 1.7799529591429746 0.000642675645405156\n",
            "4701 1.3463407127128448 0.0006409835001689394\n",
            "4801 1.4632963918847963 0.0006393046510308797\n",
            "4901 1.1903231081087142 0.0006376389247737917\n",
            "5001 1.3287691511941375 0.0006359861513233783\n",
            "5101 1.3445309301023372 0.0006343461636752915\n",
            "5201 1.5431754024625661 0.0006327187978242499\n",
            "5301 1.3343850841192761 0.0006311038926951474\n",
            "5401 1.1768817943520844 0.0006295012900760858\n",
            "5501 1.6530805606771537 0.0006279108345532683\n",
            "5601 1.2646167293241888 0.000626332373447694\n",
            "5701 1.3651119051501155 0.000624765756753594\n",
            "5801 1.831987822048177 0.0006232108370785525\n",
            "5901 1.3451470380132378 0.0006216674695852594\n",
            "6001 1.5295006221767835 0.0006201355119348414\n",
            "6101 1.2796215488779126 0.0006186148242317232\n",
            "6201 1.3307579715619795 0.0006171052689699666\n",
            "6301 1.5296110774725094 0.0006156067109810445\n",
            "1 1.355640010209754 0.0006142969713181733\n",
            "101 1.3438594869803637 0.0006128187302418007\n",
            "201 1.3398014856502414 0.0006113511097561582\n",
            "301 1.2453488917089999 0.0006098939832926246\n",
            "401 1.74672898178801 0.0006084472263842588\n",
            "501 1.348103358541266 0.0006070107166211413\n",
            "601 1.2492765338683967 0.0006055843336068713\n",
            "701 1.568915182055207 0.0006041679589161831\n",
            "801 1.3617599749704823 0.0006027614760536461\n",
            "901 1.3296397840604186 0.0006013647704134199\n",
            "1001 1.506301498040557 0.0005999777292400283\n",
            "1101 1.1846984136500396 0.000598600241590126\n",
            "1201 1.1235853107646108 0.0005972321982952243\n",
            "1301 1.3506322290195385 0.0005958734919253515\n",
            "1401 1.5431637589354068 0.0005945240167536175\n",
            "1501 1.4227895765798166 0.0005931836687216574\n",
            "1601 1.2444980588334147 0.0005918523454059284\n",
            "1701 1.37204463215312 0.000590529945984835\n",
            "1801 1.3662666375166737 0.0005892163712066582\n",
            "1901 1.758998476434499 0.0005879115233582672\n",
            "2001 1.3996043455335894 0.0005866153062345879\n",
            "2101 1.409632071852684 0.0005853276251088103\n",
            "2201 1.3139934270293452 0.0005840483867033116\n",
            "2301 1.2863777373568155 0.0005827774991612753\n",
            "2401 1.1966209802776575 0.0005815148720189864\n",
            "2501 1.3174833165830933 0.0005802604161787846\n",
            "2601 1.406668136944063 0.0005790140438826557\n",
            "2701 1.31760111481708 0.0005777756686864456\n",
            "2801 1.22686495014932 0.0005765452054346768\n",
            "2901 1.4871160766715548 0.0005753225702359537\n",
            "3001 1.3321835576352896 0.0005741076804389384\n",
            "3101 1.349290698301047 0.0005729004546088824\n",
            "3201 1.0498975263908505 0.0005717008125046992\n",
            "3301 1.4295434548403136 0.0005705086750565621\n",
            "3401 1.3862976277887356 0.0005693239643440145\n",
            "3501 1.3612052928074263 0.0005681466035745775\n",
            "3601 1.3539716337691061 0.0005669765170628427\n",
            "3701 1.3053378225304186 0.0005658136302100359\n",
            "3801 1.2067344364186283 0.0005646578694840415\n",
            "3901 1.417662046442274 0.0005635091623998715\n",
            "4001 1.2578378450434684 0.0005623674375005725\n",
            "4101 1.2363171717152 0.0005612326243385544\n",
            "4201 1.3426340871083084 0.0005601046534573332\n",
            "4301 1.3097076122212457 0.000558983456373675\n",
            "4401 1.0131576862186193 0.0005578689655601316\n",
            "4501 1.4332989812392043 0.000556761114427959\n",
            "4601 1.4043821960221976 0.0005556598373104054\n",
            "4701 1.373746110650245 0.0005545650694463629\n",
            "4801 1.2657524709356949 0.0005534767469643717\n",
            "4901 1.1224889098666608 0.0005523948068669684\n",
            "5001 1.2615516305086203 0.000551319187015369\n",
            "5101 1.409785834257491 0.0005502498261144795\n",
            "5201 1.3791224808810512 0.0005491866636982242\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "5301 1.2408291140163783 0.0005481296401151859\n",
            "5401 1.3008261130889878 0.0005470786965145471\n",
            "5501 1.1700160388209042 0.0005460337748323287\n",
            "5601 1.2999350049067289 0.000544994817777915\n",
            "5701 1.3322585223941132 0.0005439617688208604\n",
            "5801 1.254337038320955 0.0005429345721779703\n",
            "5901 1.773689029644629 0.000541913172800649\n",
            "6001 1.3898115772462916 0.0005408975163625087\n",
            "6101 1.4735579792177305 0.0005398875492472326\n",
            "6201 1.05738219874911 0.000538883218536687\n",
            "6301 1.0802461032290012 0.0005378844719992749\n",
            "1 1.286231731530279 0.0005370101533168812\n",
            "101 1.2250633136718534 0.0005360217659787991\n",
            "201 1.239320948603563 0.0005350388161199592\n",
            "301 1.4140636462761904 0.0005340612540665886\n",
            "401 1.442663955502212 0.0005330890307779102\n",
            "501 1.4505203103472013 0.0005321220978358095\n",
            "601 1.2115196966333315 0.0005311604074347066\n",
            "701 1.2035035027656704 0.0005302039123716286\n",
            "801 1.3747974793659523 0.0005292525660364788\n",
            "901 1.36490419106849 0.0005283063224024965\n",
            "1001 1.1864821948111057 0.0005273651360169036\n",
            "1101 1.1623371304303873 0.000526428961991735\n",
            "1201 1.1043747729854658 0.0005254977559948457\n",
            "1301 1.6982813560443901 0.0005245714742410941\n",
            "1401 1.2719842366641387 0.0005236500734836944\n",
            "1501 1.2951120301149786 0.0005227335110057353\n",
            "1601 1.580276207998395 0.0005218217446118628\n",
            "1701 1.218743062199792 0.0005209147326201215\n",
            "1801 1.1479590674862266 0.0005200124338539494\n",
            "1901 1.2872504810075043 0.0005191148076343284\n",
            "2001 1.8993003838438653 0.0005182218137720798\n",
            "2101 1.2762204335303977 0.0005173334125603075\n",
            "2201 1.6183682525045242 0.0005164495647669814\n",
            "2301 1.2522982619411778 0.0005155702316276618\n",
            "2401 1.2925108795752749 0.0005146953748383575\n",
            "2501 1.340747339767404 0.0005138249565485178\n",
            "2601 1.340512964350637 0.0005129589393541545\n",
            "2701 1.1672844442073256 0.0005120972862910908\n",
            "2801 1.257948145037517 0.0005112399608283344\n",
            "2901 1.510728154462413 0.0005103869268615725\n",
            "3001 1.4130934766726568 0.0005095381487067851\n",
            "3101 1.2367545471934136 0.0005086935910939762\n",
            "3201 1.3846962348325178 0.0005078532191610173\n",
            "3301 1.2582954101526411 0.0005070169984476032\n",
            "3401 1.1545094328466803 0.0005061848948893172\n",
            "3501 1.295005505089648 0.0005053568748118022\n",
            "3601 1.3319955187034793 0.0005045329049250373\n",
            "3701 1.3548947679810226 0.000503712952317716\n",
            "3801 1.4635376840888057 0.0005028969844517252\n",
            "3901 1.6542128916307774 0.0005020849691567213\n",
            "4001 1.3512894048908493 0.0005012768746248036\n",
            "4101 1.397591198408918 0.0005004726694052806\n",
            "4201 1.3055676214280538 0.0004996723223995292\n",
            "4301 1.3375271083787084 0.000498875802855943\n",
            "4401 1.2366086341207847 0.0004980830803649704\n",
            "4501 1.2439679206581786 0.0004972941248542376\n",
            "4601 1.352382222772576 0.0004965089065837576\n",
            "4701 1.7570512742054234 0.0004957273961412208\n",
            "4801 1.232903058291413 0.0004949495644373684\n",
            "4901 1.015858386293985 0.0004941753827014446\n",
            "5001 1.381107110035373 0.000493404822476726\n",
            "5101 0.9564947709441185 0.0004926378556161293\n",
            "5201 1.228621664486127 0.0004918744542778926\n",
            "5301 1.182083563413471 0.0004911145909213302\n",
            "5401 1.2583643229590962 0.0004903582383026592\n",
            "5501 1.404046923678834 0.0004896053694708976\n",
            "5601 1.2389367091745953 0.0004888559577638302\n",
            "5701 1.119320425321348 0.00048810997680404295\n",
            "5801 1.586507015679672 0.0004873674004950231\n",
            "5901 1.112720330056618 0.0004866282030173253\n",
            "6001 1.3577893248293549 0.0004858923588248005\n",
            "6101 1.217524498468265 0.0004851598426408882\n",
            "6201 1.3229771983387764 0.0004844306294549693\n",
            "6301 1.5693217546272535 0.0004837046945187796\n",
            "1 1.1786362157727126 0.00048306134975017534\n",
            "101 1.28241519164294 0.00048234154403106603\n",
            "201 1.1411214591062162 0.00048162494648183897\n",
            "301 1.2352831599419005 0.0004809115333417623\n",
            "401 1.1032181181944907 0.00048020128109574806\n",
            "501 1.18390864826506 0.00047949416647109663\n",
            "601 1.2226583541632863 0.00047879016643429347\n",
            "701 1.0373018080717884 0.0004780892581878584\n",
            "801 1.2819566036341712 0.00047739141916724456\n",
            "901 1.1648676298791543 0.0004766966270377871\n",
            "1001 1.1654199322802015 0.00047600485969170105\n",
            "1101 1.2386636545270449 0.00047531609524512704\n",
            "1201 1.2253044219105504 0.0004746303120352227\n",
            "1301 1.375744077755371 0.0004739474886173019\n",
            "1401 1.1551300736318808 0.0004732676037620178\n",
            "1501 1.5255512128351256 0.00047259063645259034\n",
            "1601 1.255034319277911 0.00047191656588207824\n",
            "1701 1.1623500876303297 0.0004712453714506923\n",
            "1801 1.2958592986833537 0.00047057703276315175\n",
            "1901 1.1341320046922192 0.0004699115296260807\n",
            "2001 1.1937441515619867 0.0004692488420454462\n",
            "2101 1.7062073841661913 0.00046858895022403485\n",
            "2201 1.2566360468044877 0.00046793183455896863\n",
            "2301 1.2216275975806639 0.0004672774756392595\n",
            "2401 1.2636524712725077 0.0004666258542434008\n",
            "2501 1.2113699619076215 0.00046597695133699556\n",
            "2601 1.1559934263350442 0.00046533074807042176\n",
            "2701 1.256740387296304 0.0004646872257765319\n",
            "2801 1.3039579528664262 0.0004640463659683885\n",
            "2901 1.2651300196012016 0.0004634081503370334\n",
            "3001 1.2652980692801066 0.000462772560749291\n",
            "3101 1.1218284339411184 0.00046213957924560355\n",
            "3201 1.2543016897689085 0.0004615091880379007\n",
            "3301 1.2131407480192138 0.0004608813695074994\n",
            "3401 1.2994702684518415 0.0004602561062030357\n",
            "3501 1.2115506358095445 0.00045963338083842724\n",
            "3601 1.1760960748360958 0.00045901317629086643\n",
            "3701 1.0682971130590886 0.00045839547559884254\n",
            "3801 1.0764332090620883 0.00045778026196019347\n",
            "3901 1.1835216325707734 0.0004571675187301866\n",
            "4001 1.3529939632862806 0.00045655722941962654\n",
            "4101 1.3684578015236184 0.0004559493776929923\n",
            "4201 1.2233722301607486 0.0004553439473666001\n",
            "4301 1.2596116681525018 0.0004547409224067939\n",
            "4401 1.2757911044172943 0.00045414028692816196\n",
            "4501 1.2199301174841821 0.0004535420251917793\n",
            "4601 1.3471774608151463 0.0004529461216034753\n",
            "4701 1.475795219448628 0.0004523525607121267\n",
            "4801 1.1835241899825633 0.0004517613272079745\n",
            "4901 1.1791616377497576 0.0004511724059209659\n",
            "5001 1.3126113665202865 0.0004505857818191191\n",
            "5101 1.2516068609402282 0.0004500014400069121\n",
            "5201 1.178165558274486 0.0004494193657236937\n",
            "5301 1.6013869942435122 0.0004488395443421177\n",
            "5401 1.2677101592962572 0.00044826196136659916\n",
            "5501 1.1976667390699731 0.0004476866024317922\n",
            "5601 1.1990807302790927 0.00044711345330108884\n",
            "5701 1.1415361673789448 0.0004465424998651406\n",
            "5801 1.2389779405202717 0.0004459737281403985\n",
            "5901 1.1746156329172663 0.0004454071242676752\n",
            "6001 1.1718775559565984 0.0004448426745107265\n",
            "6101 1.1669323876558337 0.00044428036525485275\n",
            "6201 1.22836275130976 0.0004437201830055194\n",
            "6301 1.1068585112225264 0.000443162114386997\n",
            "1 1.1908240653865505 0.00044267275186678196\n",
            "101 1.156728027795907 0.0004421186210736662\n",
            "201 1.151486962888157 0.0004415665660409348\n",
            "301 1.1075408830074593 0.0004410165738412884\n",
            "401 1.1251853418070823 0.00044046863165985925\n",
            "501 1.224421168473782 0.0004399227267929559\n",
            "601 1.1097798637929372 0.00043937884664682695\n",
            "701 0.992531725903973 0.0004388369787364407\n",
            "801 1.2762772621936165 0.0004382971106842813\n",
            "901 1.154728337773122 0.00043775923021916087\n",
            "1001 0.9699444866273552 0.00043722332517504866\n",
            "1101 1.1039727496681735 0.0004366893834899152\n",
            "1201 1.2997219555545598 0.0004361573932045913\n",
            "1301 1.5713044246076606 0.00043562734246164385\n",
            "1401 1.1782071397465188 0.00043509921950426545\n",
            "1501 1.256332863289117 0.0004345730126751789\n",
            "1601 1.162631830346072 0.00043404871041555687\n",
            "1701 1.1123517343075946 0.00043352630126395546\n",
            "1801 1.0946980192093179 0.0004330057738552615\n",
            "1901 1.120711475959979 0.0004324871169196544\n",
            "2001 1.1385652619646862 0.0004319703192815812\n",
            "2101 1.0391206528292969 0.0004314553698587452\n",
            "2201 1.1468603002722375 0.00043094225766110786\n",
            "2301 1.1944863148819422 0.0004304309717899036\n",
            "2401 1.1445480604888871 0.00042992150143666746\n",
            "2501 1.160092411795631 0.0004294138358822756\n",
            "2601 0.905779943568632 0.00042890796449599795\n",
            "2701 1.2337692737637553 0.0004284038767345632\n",
            "2801 1.2654334787439439 0.00042790156214123586\n",
            "2901 1.2613030684588011 0.0004274010103449054\n",
            "3001 1.1566388571663992 0.0004269022110591865\n",
            "3101 1.1506170178181492 0.0004264051540815317\n",
            "3201 1.1042177192866802 0.00042590982929235444\n",
            "3301 1.268968387885252 0.00042541622665416415\n",
            "3401 1.1708880871301517 0.0004249243362107117\n",
            "3501 1.1094016103306785 0.00042443414808614573\n",
            "3601 1.3188527839665767 0.0004239456524841804\n",
            "3701 1.2144307589042 0.0004234588396872726\n",
            "3801 1.1827894128946355 0.0004229737000558104\n",
            "3901 0.9924444004427642 0.00042249022402731095\n",
            "4001 1.1228576390712988 0.0004220084021156294\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "4101 1.1924936635477934 0.0004215282249101765\n",
            "4201 1.1275967326655518 0.0004210496830751471\n",
            "4301 1.0625419117277488 0.0004205727673487576\n",
            "4401 1.1389823842037003 0.00042009746854249313\n",
            "4501 1.339291847194545 0.00041962377754036395\n",
            "4601 1.0302886090357788 0.0004191516852981713\n",
            "4701 1.7122778899138211 0.00041868118284278167\n",
            "4801 1.2910672437865287 0.0004182122612714111\n",
            "4901 1.0494152382598259 0.00041774491175091685\n",
            "5001 1.1782474033534527 0.0004172791255170995\n",
            "5101 1.040663594380021 0.0004168148938740118\n",
            "5201 0.9901199785526842 0.00041635220819327733\n",
            "5301 1.5490817801910453 0.00041589105991341656\n",
            "5401 1.1346296942792833 0.00041543144053918197\n",
            "5501 1.223581779631786 0.00041497334164089994\n",
            "5601 0.958975835936144 0.0004145167548538224\n",
            "5701 1.238148811913561 0.0004140616718774844\n",
            "5801 1.1881117207813077 0.000413608084475071\n",
            "5901 1.1295715225860476 0.0004131559844727907\n",
            "6001 1.0610488005331717 0.000412705363759257\n",
            "6101 1.2371235750615597 0.00041225621428487707\n",
            "6201 1.1479767516138963 0.00041180852806124783\n",
            "6301 1.2059640220250003 0.0004113622971605593\n",
            "1 1.1765662879188312 0.0004109663692823915\n",
            "101 1.219779463717714 0.0004105228675028437\n",
            "201 0.972488499362953 0.00041008079847008953\n",
            "301 1.1617506150214467 0.0004096401544864815\n",
            "401 1.0883347367926035 0.0004092009279121472\n",
            "501 1.1085488446406089 0.00040876311116443343\n",
            "601 1.1023443718672752 0.0004083266967173559\n",
            "701 1.018611608800711 0.00040789167710105623\n",
            "801 1.1658196483003849 0.00040745804490126497\n",
            "901 1.2917855954219704 0.0004070257927587706\n",
            "1001 1.186474629881559 0.00040659491336889525\n",
            "1101 1.127356821874855 0.00040616539948097586\n",
            "1201 1.1975307842949405 0.00040573724389785204\n",
            "1301 1.1174790017685154 0.0004053104394753595\n",
            "1401 1.0863252188792103 0.0004048849791218294\n",
            "1501 1.0622602235816885 0.000404460855797593\n",
            "1601 1.1195327076129615 0.00040403806251449327\n",
            "1701 1.2447982146404684 0.00040361659233540054\n",
            "1801 1.2607627244724426 0.0004031964383737348\n",
            "1901 1.278331945562968 0.00040277759379299307\n",
            "2001 1.025594950420782 0.0004023600518062819\n",
            "2101 1.115274733179831 0.0004019438056758561\n",
            "2201 1.1167924739420414 0.0004015288487126612\n",
            "2301 1.0995151306560729 0.000401115174275883\n",
            "2401 1.1547567544039339 0.00040070277577250023\n",
            "2501 1.1590442548913416 0.00040029164665684384\n",
            "2601 1.1047132272506133 0.00039988178043016053\n",
            "2701 1.0620037270709872 0.00039947317064018093\n",
            "2801 1.0939110746548977 0.00039906581088069363\n",
            "2901 0.9693534299731255 0.0003986596947911227\n",
            "3001 1.2754340882529505 0.0003982548160561108\n",
            "3101 2.0011723663365046 0.0003978511684051071\n",
            "3201 1.1672507325711194 0.0003974487456119586\n",
            "3301 0.8547125565819442 0.00039704754149450736\n",
            "3401 1.0948779656609986 0.00039664754991419163\n",
            "3501 1.1662541554399013 0.0003962487647756509\n",
            "3601 1.242357063729287 0.00039585118002633614\n",
            "3701 1.142975198366912 0.000395454789656124\n",
            "3801 1.2055247909738682 0.0003950595876969351\n",
            "3901 1.2129587295930833 0.0003946655682223565\n",
            "4001 1.239052205113694 0.0003942727253472687\n",
            "4101 1.1285374546132516 0.0003938810532274764\n",
            "4201 1.3123125492420513 0.00039349054605934306\n",
            "4301 1.2088057449145708 0.00039310119807943006\n",
            "4401 1.0897887839237228 0.00039271300356413926\n",
            "4501 1.2131327866227366 0.00039232595682935973\n",
            "4601 0.9877158605959266 0.000391940052230118\n",
            "4701 1.2145014504967548 0.0003915552841602323\n",
            "4801 1.1513765653944574 0.0003911716470519708\n",
            "4901 1.1751896993955597 0.0003907891353757127\n",
            "5001 1.1771067604749987 0.0003904077436396139\n",
            "5101 1.3224336538114585 0.0003900274663892758\n",
            "5201 1.355893952131737 0.0003896482982074174\n",
            "5301 1.1996791153214872 0.0003892702337135512\n",
            "5401 1.206806231304654 0.0003888932675636631\n",
            "5501 1.280991047504358 0.0003885173944498942\n",
            "5601 0.9168080711970106 0.0003881426091002278\n",
            "5701 1.1924245768459514 0.0003877689062781782\n",
            "5801 1.1940782586170826 0.0003873962807824836\n",
            "5901 1.2784329915011767 0.0003870247274468023\n",
            "6001 1.1448089724872261 0.00038665424113941134\n",
            "6101 1.1181278676813236 0.0003862848167629092\n",
            "6201 1.2686003018752672 0.00038591644925392126\n",
            "6301 1.0795898175565526 0.000385549133582808\n",
            "1 1.199639980099164 0.00038523407955521927\n",
            "101 1.0967486363369972 0.00038486870703897236\n",
            "201 1.2039306252845563 0.00038450437215947677\n",
            "301 1.106695241353009 0.0003841410700146326\n",
            "401 1.0133273621067929 0.00038377879573470126\n",
            "501 1.1209047999582253 0.0003834175444820315\n",
            "601 1.073817516444251 0.00038305731145078797\n",
            "701 1.1002086726948619 0.00038269809186668256\n",
            "801 1.044247637852095 0.00038233988098670897\n",
            "901 1.1538543488713913 0.00038198267409887953\n",
            "1001 1.1638364950194955 0.00038162646652196454\n",
            "1101 1.1222136826545466 0.00038127125360523515\n",
            "1201 1.0936923912668135 0.0003809170307282081\n",
            "1301 1.0790816302178428 0.0003805637933003932\n",
            "1401 1.0973517978854943 0.0003802115367610436\n",
            "1501 1.3543376340385294 0.00037986025657890806\n",
            "1601 1.0638599513913505 0.0003795099482519871\n",
            "1701 1.095864930888638 0.0003791606073072896\n",
            "1801 1.3785420355270617 0.00037881222930059356\n",
            "1901 1.0656100183841772 0.0003784648098162084\n",
            "2001 1.083574770949781 0.0003781183444667399\n",
            "2101 1.7192173111798184 0.0003777728288928577\n",
            "2201 1.090653446619399 0.0003774282587630644\n",
            "2301 1.1122783308528597 0.00037708462977346826\n",
            "2401 0.95696423901245 0.0003767419376475568\n",
            "2501 1.2160079335735645 0.0003764001781359734\n",
            "2601 1.2086039673304185 0.00037605934701629616\n",
            "2701 1.0832101813866757 0.00037571944009281874\n",
            "2801 1.013074157119263 0.00037538045319633314\n",
            "2901 1.0823434699559584 0.00037504238218391556\n",
            "3001 1.1612105248786975 0.0003747052229387128\n",
            "3101 0.9126896761590615 0.0003743689713697328\n",
            "3201 1.3341417479550728 0.00037403362341163505\n",
            "3301 0.9600269035436213 0.0003736991750245252\n",
            "3401 1.1916928359714802 0.0003733656221937497\n",
            "3501 1.4976106537505984 0.0003730329609296942\n",
            "3601 1.1840611910447478 0.0003727011872675824\n",
            "3701 1.3150727476167958 0.0003723702972672783\n",
            "3801 1.221188339870423 0.00037204028701308904\n",
            "3901 1.2026138188084587 0.0003717111526135708\n",
            "4001 1.3793403076779214 0.00037138289020133557\n",
            "4101 1.0772470782976598 0.0003710554959328607\n",
            "4201 1.0168679640773917 0.0003707289659882998\n",
            "4301 1.1685322925950459 0.00037040329657129513\n",
            "4401 1.1224966624286026 0.00037007848390879306\n",
            "4501 1.0253048577578738 0.00036975452425085955\n",
            "4601 1.2101853350850433 0.00036943141387049916\n",
            "4701 1.050108958443161 0.0003691091490634741\n",
            "4801 1.2717001468117815 0.00036878772614812674\n",
            "4901 1.1231291381409392 0.00036846714146520227\n",
            "5001 1.1141781120968517 0.00036814739137767423\n",
            "5101 0.9994667179416865 0.00036782847227057074\n",
            "5201 1.3557415267750912 0.0003675103805508032\n",
            "5301 1.504937146051816 0.0003671931126469962\n",
            "5401 1.0444834220979828 0.00036687666500931896\n",
            "5501 1.0159150707913795 0.0003665610341093186\n",
            "5601 1.4691102042561397 0.00036624621643975515\n",
            "5701 1.2679062836105004 0.0003659322085144373\n",
            "5801 1.1070539963402553 0.0003656190068680607\n",
            "5901 1.2043958652066067 0.0003653066080560474\n",
            "6001 1.1217296464601532 0.00036499500865438625\n",
            "6101 1.2132740695233224 0.00036468420525947586\n",
            "6201 1.452793362134571 0.00036437419448796804\n",
            "6301 1.0731251265387982 0.00036406497297661317\n",
            "1 1.1998733360724145 0.00036379350826718935\n",
            "101 1.1498671763110906 0.0003634857615296514\n",
            "201 1.1022596344992053 0.00036317879447701637\n",
            "301 1.0011312331771478 0.00036287260382257964\n",
            "401 1.0373523531015962 0.0003625671862990008\n",
            "501 1.0644397677097004 0.000362262538658157\n",
            "601 1.1183270415349398 0.000361958657670998\n",
            "701 0.9439565273642074 0.00036165554012740277\n",
            "801 1.0483181119780056 0.0003613531828360362\n",
            "901 1.10791165966657 0.00036105158262420917\n",
            "1001 0.9895736404578201 0.00036075073633773743\n",
            "1101 1.0942751504917396 0.00036045064084080426\n",
            "1201 1.0274720180314034 0.0003601512930158222\n",
            "1301 1.049829078532639 0.0003598526897632977\n",
            "1401 1.0599873741302872 0.00035955482800169595\n",
            "1501 1.1110646773013286 0.00035925770466730756\n",
            "1601 1.1195740707335062 0.0003589613167141159\n",
            "1701 1.1175601889844984 0.0003586656611136664\n",
            "1801 1.27650167158572 0.00035837073485493607\n",
            "1901 1.1153064398095012 0.000358076534944205\n",
            "2001 1.4756392514391337 0.000357783058404929\n",
            "2101 0.8967400579713285 0.0003574903022776121\n",
            "2201 1.0554919667192735 0.0003571982636196827\n",
            "2301 1.1484477042686194 0.000356906939505368\n",
            "2401 1.0967925000586547 0.00035661632702557175\n",
            "2501 1.275310180048109 0.0003563264232877516\n",
            "2601 1.084061863599345 0.00035603722541579873\n",
            "2701 1.076280384673737 0.00035574873054991784\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2801 1.200010517553892 0.0003554609358465082\n",
            "2901 1.059172057226533 0.000355173838478046\n",
            "3001 1.0261963941156864 0.000354887435632968\n",
            "3101 0.9737269952893257 0.0003546017245155551\n",
            "3201 1.109491402952699 0.0003543167023458187\n",
            "3301 1.0379895093501545 0.0003540323663593864\n",
            "3401 1.0210047286818735 0.00035374871380738974\n",
            "3501 1.3062616163679195 0.0003534657419563522\n",
            "3601 1.1297821700572968 0.00035318344808807914\n",
            "3701 1.095454223890556 0.0003529018294995477\n",
            "3801 1.0263875699602067 0.00035262088350279793\n",
            "3901 1.0200049103004858 0.00035234060742482575\n",
            "4001 1.139240143907955 0.00035206099860747537\n",
            "4101 1.185085133272878 0.00035178205440733397\n",
            "4201 1.0887831579057092 0.0003515037721956263\n",
            "4301 1.7533796445081862 0.000351226149358111\n",
            "4401 1.1149956742010545 0.00035094918329497705\n",
            "4501 1.1544227619015146 0.0003506728714207421\n",
            "4601 1.1121961249154992 0.00035039721116415036\n",
            "4701 1.0929120010696352 0.0003501221999680728\n",
            "4801 1.118996370700188 0.000349847835289407\n",
            "4901 0.9860638748505153 0.00034957411459897886\n",
            "5001 1.004449057742022 0.0003493010353814441\n",
            "5101 1.2927988782757893 0.00034902859513519165\n",
            "5201 0.98420900356723 0.0003487567913722472\n",
            "5301 1.1210692654858576 0.000348485621618178\n",
            "5401 1.163566045666812 0.00034821508341199766\n",
            "5501 1.17701395630138 0.0003479451743060731\n",
            "5601 1.0291424575298151 0.00034767589186603104\n",
            "5701 1.2543358486509533 0.0003474072336706659\n",
            "5801 1.2299754508348997 0.00034713919731184855\n",
            "5901 1.1936145080917413 0.0003468717803944353\n",
            "6001 0.9138605990447104 0.00034660498053617827\n",
            "6101 1.1037570714397589 0.00034633879536763624\n",
            "6201 1.04157008238235 0.00034607322253208626\n",
            "6301 1.1919174637878314 0.0003458082596854357\n",
            "1 0.9797578346915543 0.00034557559511139293\n",
            "101 0.9891712895187084 0.00034531177274179953\n",
            "201 1.1815550197497942 0.00034504855367990236\n",
            "301 1.1358435613219626 0.0003447859356297997\n",
            "401 1.0717519058380276 0.000344523916307803\n",
            "501 1.172375235328218 0.00034426249344235384\n",
            "601 1.0603420100815129 0.00034400166477394084\n",
            "701 1.0992282917286502 0.000343741428055018\n",
            "801 1.04559408465866 0.0003434817810499231\n",
            "901 1.0248865495998416 0.0003432227215347973\n",
            "1001 1.0598302248690743 0.0003429642472975047\n",
            "1101 1.0938185814011376 0.0003427063561375535\n",
            "1201 1.291374852447916 0.000342449045866017\n",
            "1301 1.0285806620959193 0.0003421923143054557\n",
            "1401 1.17992360109929 0.0003419361592898398\n",
            "1501 0.9615428688703105 0.0003416805786644727\n",
            "1601 1.2486475716141285 0.0003414255702859144\n",
            "1701 0.9794061238644645 0.0003411711320219065\n",
            "1801 1.1059001302346587 0.00034091726175129706\n",
            "1901 0.9131126408465207 0.00034066395736396637\n",
            "2001 0.9930663524428383 0.0003404112167607534\n",
            "2101 1.0812218267819844 0.0003401590378533823\n",
            "2201 1.0715046060213353 0.0003399074185643907\n",
            "2301 1.1185214965953492 0.00033965635682705713\n",
            "2401 1.05721441534115 0.00033940585058533\n",
            "2501 1.0936700437378022 0.00033915589779375693\n",
            "2601 1.07034515045234 0.00033890649641741454\n",
            "2701 1.0813248989579733 0.00033865764443183875\n",
            "2801 1.0510470166627783 0.0003384093398229561\n",
            "2901 0.9623011860530823 0.0003381615805870148\n",
            "3001 1.1494725269731134 0.00033791436473051725\n",
            "3101 1.2335324875239166 0.0003376676902701525\n",
            "3201 1.0398004396120086 0.00033742155523272933\n",
            "3301 1.0589452146668918 0.0003371759576551101\n",
            "3401 1.084106142167002 0.00033693089558414497\n",
            "3501 1.0406659920408856 0.0003366863670766065\n",
            "3601 0.9325393754988909 0.00033644237019912526\n",
            "3701 1.0507557194505353 0.0003361989030281253\n",
            "3801 0.945562198292464 0.0003359559636497606\n",
            "3901 1.0489263081690297 0.0003357135501598519\n",
            "4001 1.0528855019947514 0.00033547166066382383\n",
            "4101 1.1952165458351374 0.0003352302932766432\n",
            "4201 1.0958911241032183 0.00033498944612275674\n",
            "4301 1.077680416405201 0.0003347491173360301\n",
            "4401 1.2972540742484853 0.0003345093050596873\n",
            "4501 1.039087069220841 0.0003342700074462501\n",
            "4601 0.9597453814931214 0.00033403122265747876\n",
            "4701 1.1728105103829876 0.00033379294886431207\n",
            "4801 1.1258336059836438 0.0003335551842468092\n",
            "4901 1.0011352995352354 0.0003333179269940906\n",
            "5001 0.9672558718448272 0.00033308117530428074\n",
            "5101 1.0522874586749822 0.0003328449273844502\n",
            "5201 1.0063609674107283 0.0003326091814505589\n",
            "5301 1.0480196221014921 0.00033237393572739917\n",
            "5401 1.0445173801199417 0.00033213918844854004\n",
            "5501 1.417743748796056 0.00033190493785627127\n",
            "5601 1.0902981872641249 0.000331671182201548\n",
            "5701 1.0301871165866032 0.00033143791974393625\n",
            "5801 1.4090074983541854 0.00033120514875155805\n",
            "5901 1.1904211936052889 0.0003309728675010378\n",
            "6001 1.1052953382313717 0.0003307410742774485\n",
            "6101 1.133972127106972 0.00033050976737425853\n",
            "6201 1.4820798086614104 0.00033027894509327907\n",
            "6301 1.1476092239608988 0.00033004860574461153\n",
            "1 1.0870586273958907 0.00032984630526377065\n",
            "101 1.00110832543578 0.00032961686928176145\n",
            "201 0.9876259700540686 0.0003293879114110055\n",
            "301 1.2413200094233616 0.0003291594299932822\n",
            "401 0.9424758276436478 0.00032893142337841173\n",
            "501 1.0926933737646323 0.00032870388992420444\n",
            "601 1.0706572374765528 0.0003284768279964114\n",
            "701 1.0879125816572923 0.00032825023596867546\n",
            "801 1.262531905740616 0.0003280241122224816\n",
            "901 1.050877535046311 0.0003277984551471088\n",
            "1001 1.135452825037646 0.0003275732631395822\n",
            "1101 1.0099836179433623 0.0003273485346046242\n",
            "1201 1.1641883124248125 0.0003271242679546084\n",
            "1301 1.0249766572378576 0.00032690046160951133\n",
            "1401 1.216345368164184 0.0003266771139968662\n",
            "1501 1.4009095890432945 0.00032645422355171653\n",
            "1601 0.9214687866624445 0.00032623178871657\n",
            "1701 1.050587208737852 0.0003260098079413526\n",
            "1801 1.0106142781150993 0.0003257882796833635\n",
            "1901 0.9388233295176178 0.00032556720240723\n",
            "2001 1.1458081254386343 0.0003253465745848626\n",
            "2101 1.0818304931126477 0.00032512639469541087\n",
            "2201 0.8635702040046453 0.0003249066612252194\n",
            "2301 1.0180434776411857 0.00032468737266778394\n",
            "2401 1.0467977939988486 0.0003244685275237081\n",
            "2501 1.083000476603047 0.0003242501243006605\n",
            "2601 1.1069668279960752 0.00032403216151333166\n",
            "2701 1.086160118225962 0.00032381463768339173\n",
            "2801 1.0924749624973629 0.0003235975513394485\n",
            "2901 1.009744831302669 0.00032338090101700554\n",
            "3001 0.9085385013604537 0.0003231646852584205\n",
            "3101 1.1706041378201917 0.00032294890261286426\n",
            "3201 1.032271361502353 0.00032273355163627964\n",
            "3301 1.2584509218577296 0.00032251863089134133\n",
            "3401 1.2436874122358859 0.000322304138947415\n",
            "3501 1.0451832013077365 0.0003220900743805179\n",
            "3601 1.0900762653473066 0.00032187643577327854\n",
            "3701 1.1192542002827395 0.00032166322171489793\n",
            "3801 1.00253647408681 0.0003214504308011099\n",
            "3901 1.2160937447333708 0.0003212380616341424\n",
            "4001 1.0416435159859248 0.0003210261128226793\n",
            "4101 1.3598752447869629 0.00032081458298182156\n",
            "4201 1.0555532689650136 0.0003206034707330495\n",
            "4301 1.1295962483854964 0.00032039277470418526\n",
            "4401 0.9410244208120275 0.0003201824935293548\n",
            "4501 0.8939700378105044 0.00031997262584895135\n",
            "4601 0.908640876179561 0.000319763170309598\n",
            "4701 1.128680162204546 0.0003195541255641112\n",
            "4801 1.0497526655672118 0.00031934549027146444\n",
            "4901 1.0289937005145475 0.0003191372630967521\n",
            "5001 0.9918764412868768 0.0003189294427111535\n",
            "5101 1.2255524442298338 0.0003187220277918973\n",
            "5201 1.3292681298672733 0.0003185150170222263\n",
            "5301 0.878005885053426 0.00031830840909136197\n",
            "5401 1.0740752452165907 0.00031810220269447\n",
            "5501 1.0994656120092259 0.00031789639653262544\n",
            "5601 1.159670107124839 0.0003176909893127784\n",
            "5701 0.8859041188843548 0.0003174859797477199\n",
            "5801 1.084522244927939 0.00031728136655604814\n",
            "5901 1.4824702723776682 0.0003170771484621346\n",
            "6001 1.230977819112013 0.0003168733241960908\n",
            "6101 1.0119965468620649 0.00031666989249373517\n",
            "6201 1.1002646164814678 0.00031646685209656003\n",
            "6301 1.1440792203939054 0.00031626420175169897\n",
            "1 1.0551144047021808 0.0003160882122689669\n",
            "101 1.0408390048833098 0.00031588628797931984\n",
            "201 1.0153360446565785 0.0003156847501772966\n",
            "301 1.01748421555385 0.0003154835976315582\n",
            "401 1.1267781729111448 0.0003152828291162512\n",
            "501 1.0327468327741371 0.0003150824434109757\n",
            "601 0.9960314880299848 0.0003148824393007546\n",
            "701 1.0631007702104398 0.00031468281557600267\n",
            "801 1.0372768385277595 0.00031448357103249544\n",
            "901 1.031023440795252 0.0003142847044713392\n",
            "1001 0.9323838343843818 0.0003140862146989404\n",
            "1101 0.850510573014617 0.0003138881005269756\n",
            "1201 1.0943628003296908 0.0003136903607723615\n",
            "1301 1.0844742289336864 0.00031349299425722566\n",
            "1401 1.4024500491796061 0.00031329599980887637\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1501 1.1463393379817717 0.00031309937625977405\n",
            "1601 1.0650637014914537 0.0003129031224475018\n",
            "1701 1.1410465109511279 0.00031270723721473664\n",
            "1801 1.0148204645956866 0.0003125117194092209\n",
            "1901 0.9743651752360165 0.0003123165678837336\n",
            "2001 0.990701739974611 0.00031212178149606226\n",
            "2101 1.1128740338463103 0.00031192735910897496\n",
            "2201 1.5508626039809315 0.0003117332995901923\n",
            "2301 1.01486110695987 0.00031153960181235955\n",
            "2401 0.9611194784665713 0.0003113462646530196\n",
            "2501 1.0847897573257796 0.0003111532869945851\n",
            "2601 1.2803321699175285 0.00031096066772431187\n",
            "2701 1.0769891024538083 0.0003107684057342714\n",
            "2801 1.0039243546780199 0.00031057649992132457\n",
            "2901 1.0824949400266632 0.00031038494918709473\n",
            "3001 0.9644128995714709 0.00031019375243794144\n",
            "3101 0.9868561172188492 0.00031000290858493437\n",
            "3201 1.0903575613629073 0.00030981241654382685\n",
            "3301 1.2633273452374851 0.0003096222752350304\n",
            "3401 1.0088038056419464 0.000309432483583589\n",
            "3501 1.1047235757578164 0.0003092430405191533\n",
            "3601 1.163446888080216 0.00030905394497595545\n",
            "3701 1.018205283649877 0.00030886519589278384\n",
            "3801 1.0408570388099179 0.00030867679221295824\n",
            "3901 1.0657447287230752 0.00030848873288430483\n",
            "4001 0.930832964291767 0.0003083010168591314\n",
            "4101 1.4587874389944773 0.00030811364309420327\n",
            "4201 1.1227497690124437 0.0003079266105507184\n",
            "4301 1.1970081577601377 0.0003077399181942835\n",
            "4401 1.1083186157047749 0.00030755356499488986\n",
            "4501 1.0473160178516991 0.00030736754992688985\n",
            "4601 1.0928417469840497 0.0003071818719689727\n",
            "4701 1.0073067757184617 0.00030699653010414117\n",
            "4801 1.1523846584532293 0.00030681152331968824\n",
            "4901 1.1988015054084826 0.0003066268506071739\n",
            "5001 1.036811558995396 0.00030644251096240176\n",
            "5101 1.0675939484208357 0.0003062585033853964\n",
            "5201 1.0752534797684348 0.00030607482688038056\n",
            "5301 1.1183025861246279 0.0003058914804557523\n",
            "5401 1.0365500289481133 0.0003057084631240629\n",
            "5501 1.129350705537945 0.00030552577390199393\n",
            "5601 1.06671357084997 0.0003053434118103358\n",
            "5701 1.0859052878222428 0.0003051613758739652\n",
            "5801 1.0723684425465763 0.00030497966512182316\n",
            "5901 1.0603833084605867 0.0003047982785868937\n",
            "6001 1.0581669194652932 0.0003046172153061818\n",
            "6101 1.2675022858026068 0.0003044364743206923\n",
            "6201 1.0536423055746127 0.0003042560546754083\n",
            "6301 1.2465427681345318 0.00030407595541926996\n",
            "1 0.8441335130482912 0.00030391413923858634\n",
            "101 0.9350836968515068 0.00030373464611573535\n",
            "201 1.0421846130630001 0.0003035554706461405\n",
            "301 1.008769184758421 0.0003033766118939749\n",
            "401 0.9787611065548845 0.000303198068927267\n",
            "501 1.0469113910803571 0.00030301984081788013\n",
            "601 1.0306272330635693 0.00030284192664149214\n",
            "701 0.8391264111269265 0.00030266432547757535\n",
            "801 0.9852646276758605 0.00030248703640937665\n",
            "901 0.9640902730752714 0.00030231005852389745\n",
            "1001 1.1280414578068303 0.00030213339091187405\n",
            "1101 0.9939612101297826 0.0003019570326677579\n",
            "1201 1.0867023059399799 0.00030178098288969626\n",
            "1301 1.0411206257122103 0.00030160524067951265\n",
            "1401 1.0711584523378406 0.0003014298051426879\n",
            "1501 1.0796883448783774 0.0003012546753883405\n",
            "1601 1.027666941517964 0.00030107985052920836\n",
            "1701 1.082986782770604 0.00030090532968162913\n",
            "1801 1.1074479344606516 0.0003007311119655219\n",
            "1901 1.1305997944582487 0.0003005571965043686\n",
            "2001 1.345339710366943 0.0003003835824251953\n",
            "2101 1.001590578132891 0.00030021026885855383\n",
            "2201 1.0054450589232147 0.0003000372549385033\n",
            "2301 1.0041432639409322 0.00029986453980259265\n",
            "2401 1.0640304164699046 0.00029969212259184163\n",
            "2501 1.1201181028736755 0.0002995200024507235\n",
            "2601 0.8765224074013531 0.00029934817852714696\n",
            "2701 1.0152945614827331 0.0002991766499724386\n",
            "2801 1.2956223709957158 0.0002990054159413251\n",
            "2901 1.097986907014274 0.0002988344755919157\n",
            "3001 1.2291080165232415 0.00029866382808568526\n",
            "3101 1.1140619127836544 0.0002984934725874564\n",
            "3201 1.0722678579004423 0.0002983234082653825\n",
            "3301 1.03946516571159 0.0002981536342909311\n",
            "3401 0.9876702070032479 0.0002979841498388662\n",
            "3501 0.8540887358831242 0.00029781495408723205\n",
            "3601 0.9894529741141014 0.00029764604621733594\n",
            "3701 1.0710785342380404 0.000297477425413732\n",
            "3801 1.2710673977526312 0.00029730909086420423\n",
            "3901 1.0929949116252828 0.0002971410417597504\n",
            "4001 1.1222996068827342 0.0002969732772945655\n",
            "4101 1.164539644116303 0.00029680579666602566\n",
            "4201 1.033734397671651 0.000296638599074672\n",
            "4301 1.093015514779836 0.0002964716837241944\n",
            "4401 1.0481613585725427 0.0002963050498214161\n",
            "4501 1.0937337041832507 0.00029613869657627706\n",
            "4601 1.1815662420112858 0.000295972623201819\n",
            "4701 1.1428916620570817 0.0002958068289141693\n",
            "4801 1.1009264337189961 0.0002956413129325257\n",
            "4901 0.8777621657354757 0.00029547607447914055\n",
            "5001 1.0156730558082927 0.00029531111277930595\n",
            "5101 0.9942513670539483 0.00029514642706133804\n",
            "5201 1.1302866424011881 0.00029498201655656206\n",
            "5301 1.0087051462905947 0.0002948178804992971\n",
            "5401 1.0660143050336046 0.0002946540181268415\n",
            "5501 1.0107977577135898 0.00029449042867945755\n",
            "5601 1.0777363086963305 0.0002943271114003569\n",
            "5701 0.856828257907182 0.00029416406553568584\n",
            "5801 1.1287360956775956 0.0002940012903345107\n",
            "5901 1.366358119645156 0.00029383878504880313\n",
            "6001 1.0964845723065082 0.00029367654893342604\n",
            "6101 1.34646458978159 0.00029351458124611887\n",
            "6201 1.2197050878312439 0.0002933528812474836\n",
            "6301 1.0830936049751472 0.0002931914482009704\n",
            "1 1.2537849597129025 0.00029304477550482497\n",
            "101 0.9655292083625682 0.00029288385030021\n",
            "201 1.1372923650196753 0.0002927231899204302\n",
            "301 0.9451354363700375 0.0002925627936399378\n",
            "401 0.9661671929707154 0.00029240266073596516\n",
            "501 1.0162687979172915 0.0002922427904885108\n",
            "601 0.9551542007829994 0.0002920831821803257\n",
            "701 0.9841917234880384 0.0002919238350969\n",
            "801 1.061543255826109 0.00029176474852644945\n",
            "901 0.985908080736408 0.0002916059217599022\n",
            "1001 1.0337736615701942 0.0002914473540908853\n",
            "1101 0.9899544979416532 0.0002912890448157118\n",
            "1201 1.1052642236463726 0.00029113099323336726\n",
            "1301 0.9609193275682628 0.00029097319864549706\n",
            "1401 1.0143777604680508 0.0002908156603563932\n",
            "1501 1.0131031578639522 0.0002906583776729816\n",
            "1601 1.0399196342332289 0.00029050134990480915\n",
            "1701 1.3567781529854983 0.00029034457636403104\n",
            "1801 1.1145936762022757 0.0002901880563653981\n",
            "1901 1.0984270876506343 0.0002900317892262443\n",
            "2001 1.0226630433771788 0.00028987577426647405\n",
            "2101 1.1856945900362916 0.0002897200108085499\n",
            "2201 1.125245438015554 0.00028956449817748025\n",
            "2301 1.126162831991678 0.00028940923570080693\n",
            "2401 0.971063018507266 0.00028925422270859307\n",
            "2501 0.8773902256507427 0.00028909945853341086\n",
            "2601 0.8763325407635421 0.0002889449425103295\n",
            "2701 1.1415061227562546 0.0002887906739769035\n",
            "2801 1.052460735765635 0.0002886366522731603\n",
            "2901 1.5205009760629764 0.00028848287674158846\n",
            "3001 0.9414957111439435 0.0002883293467271265\n",
            "3101 0.9435001520323567 0.0002881760615771502\n",
            "3201 1.1403545759221743 0.0002880230206414618\n",
            "3301 1.053262686386006 0.00028787022327227786\n",
            "3401 1.0832857484929264 0.000287717668824218\n",
            "3501 1.1019753235159442 0.00028756535665429354\n",
            "3601 1.055358653771691 0.0002874132861218958\n",
            "3701 1.0278627741499804 0.00028726145658878504\n",
            "3801 1.0083879251906183 0.0002871098674190792\n",
            "3901 1.0555589499126654 0.0002869585179792425\n",
            "4001 1.0509156602493022 0.00028680740763807453\n",
            "4101 1.0385481148259714 0.0002866565357666993\n",
            "4201 1.0417402729653986 0.0002865059017385537\n",
            "4301 1.082753369351849 0.0002863555049293774\n",
            "4401 1.0459589868987678 0.0002862053447172013\n",
            "4501 1.0592919351911405 0.00028605542048233684\n",
            "4601 1.1034397517796606 0.0002859057316073656\n",
            "4701 1.1311876591207692 0.00028575627747712837\n",
            "4801 1.070465801298269 0.00028560705747871445\n",
            "4901 1.199700104945805 0.00028545807100145134\n",
            "5001 1.2379299406893551 0.00028530931743689397\n",
            "5101 1.1045849512156565 0.00028516079617881457\n",
            "5201 0.9958119990806154 0.000285012506623192\n",
            "5301 1.620139996672151 0.00028486444816820157\n",
            "5401 1.0613090786646353 0.0002847166202142048\n",
            "5501 1.175794189737644 0.0002845690221637393\n",
            "5601 1.1241089710965753 0.00028442165342150834\n",
            "5701 1.160597581154434 0.000284274513394371\n",
            "5801 1.0310369414401066 0.0002841276014913322\n",
            "5901 0.9960121748881647 0.0002839809171235324\n",
            "6001 0.9698299318188219 0.00028383445970423817\n",
            "6101 1.1415085992775857 0.0002836882286488319\n",
            "6201 1.0641657677479088 0.0002835422233748022\n",
            "6301 1.0828722650112468 0.00028339644330173413\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35JC6i9QUgSB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "1 10.825187489390373 6.987712429686844e-07\n",
        "101 9.447168171405792 3.56373333914029e-05\n",
        "201 7.142856806516647 7.057589553983712e-05\n",
        "301 6.237934365868568 0.00010551445768827134\n",
        "401 5.762486848048866 0.00014045301983670557\n",
        "501 5.415792358107865 0.00017539158198513977\n",
        "601 5.081815680023283 0.000210330144133574\n",
        "701 4.788327748770826 0.00024526870628200823\n",
        "801 4.381739928154275 0.0002802072684304424\n",
        "901 4.55433791608084 0.00031514583057887664\n",
        "1001 4.911875109748507 0.0003500843927273108\n",
        "1101 4.0579032292589545 0.0003850229548757451\n",
        "1201 4.2276234351193125 0.0004199615170241793\n",
        "1301 3.932735869428143 0.00045490007917261356\n",
        "1401 3.8179439397063106 0.0004898386413210477\n",
        "1501 3.3608515430241823 0.000524777203469482\n",
        "1601 3.832796103321016 0.0005597157656179162\n",
        "1701 2.907085266895592 0.0005946543277663504\n",
        "1801 3.5280659823838505 0.0006295928899147847\n",
        "1901 2.895841649500653 0.0006645314520632189\n",
        "2001 3.273784235585481 0.000699470014211653\n",
        "2101 3.181488689899197 0.0007344085763600873\n",
        "2201 3.4151616653980454 0.0007693471385085215\n",
        "2301 3.4343731447588652 0.0008042857006569557\n",
        "2401 3.0505455391539726 0.0008392242628053899\n",
        "2501 2.8089329147478566 0.0008741628249538242\n",
        "2601 2.7827929875456903 0.0009091013871022583\n",
        "2701 2.4428516102489084 0.0009440399492506926\n",
        "2801 2.4015486147254705 0.0009789785113991267\n",
        "2901 2.3568112018401735 0.001013917073547561\n",
        "3001 2.6349758653668687 0.0010488556356959952\n",
        "3101 2.5981983028614195 0.0010837941978444295\n",
        "3201 2.666826274838968 0.0011187327599928637\n",
        "3301 3.0092043554177508 0.0011536713221412978\n",
        "3401 2.4580375660589198 0.0011886098842897321\n",
        "3501 2.586465588421561 0.0012235484464381662\n",
        "3601 2.5663993963389657 0.0012584870085866006\n",
        "3701 2.9430236657499336 0.0012934255707350347\n",
        "3801 2.464644919440616 0.001328364132883469\n",
        "3901 2.7124062888276512 0.0013633026950319032\n",
        "4001 2.646443709731102 0.0013971932312809247\n",
        "4101 2.7294750874862075 0.001380057517579748\n",
        "4201 2.1295202329056337 0.0013635372009002666\n",
        "4301 2.596563663915731 0.001347596306985731\n",
        "4401 2.1265982036820787 0.0013322017384983986\n",
        "4501 2.3880532500334084 0.0013173229858148\n",
        "4601 2.6129120760888327 0.0013029318725783852\n",
        "4701 2.2873719420749694 0.001289002331178292\n",
        "4801 2.4949760700110346 0.0012755102040816328\n",
        "4901 2.496607314562425 0.001262433067573089\n",
        "5001 2.1889712483389303 0.0012497500749750088\n",
        "5101 1.8677761815488338 0.0012374418168536253\n",
        "5201 2.2992054556962103 0.0012254901960784316\n",
        "5301 2.664361578106707 0.0012138783159049418\n",
        "5401 2.705850490485318 0.0012025903795063202\n",
        "5501 2.581445264921058 0.0011916115995949978\n",
        "5601 2.2480602325085783 0.0011809281169581616\n",
        "5701 1.9289666265249252 0.0011705269268863989\n",
        "5801 2.4863578918157145 0.0011603958126073107\n",
        "5901 2.632946971571073 0.0011505232849492607\n",
        "6001 2.496141305891797 0.0011408985275576757\n",
        "6101 2.6422974687084206 0.0011315113470699342\n",
        "6201 2.448802186456305 0.0011223521277270118"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}